{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizawang/PCL/blob/main/pcl_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npeYpqYQ1mYS",
        "outputId": "2b652b0a-26dd-4528-bad5-c650d63e6d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/PCL\n",
            "\u001b[0m\u001b[01;34m10-langs-subtask1_all\u001b[0m/            raw.pickle\n",
            "5nn_predictions.csv               README.txt\n",
            "\u001b[01;34mbacktranslation_subtask2\u001b[0m/         rebuilddata.py\n",
            "data1                             \u001b[01;34mref\u001b[0m/\n",
            "data1_augmented.csv               \u001b[01;34mres\u001b[0m/\n",
            "data1.pickle                      sentiment_scoring.py\n",
            "data2.pickle                      task4_test.tsv\n",
            "\u001b[01;34mDeepMoji\u001b[0m/                         ten-langs-regex-cleaned-binary-label.csv\n",
            "dev_semeval_parids-labels.csv     \u001b[01;34mthePoorerTheMerrier\u001b[0m/\n",
            "dontpatronizeme_categories.tsv    tptm.csv\n",
            "dontpatronizeme_pcl.tsv           train_semeval_parids-labels.csv\n",
            "dont_patronize_me.py              trn_5nn_predictions.csv\n",
            "emoji_feature_vectors_trn_x1.csv  trndf1.csv\n",
            "emoji_feature_vectors_tst_x1.csv  trndf2.csv\n",
            "evaluation.py                     trn_encoded_with_senti_df1\n",
            "model_graph.png                   trn_x1_augmented\n",
            "\u001b[01;34mmodelsForStack\u001b[0m/                   trn_x1_original\n",
            "pcl.ipynb                         tstdf1.csv\n",
            "pcl-task1-augment-revised.ipynb   tstdf2.csv\n",
            "pcl-task2-augment.ipynb           tst_x1_original\n",
            "\u001b[01;34m__pycache__\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/PCL/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQm_NkiyHKII",
        "outputId": "b48601a7-45e9-4a12-b9fd-5f99eff52abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=efedb63f34446195ff07451ad5d4586e77c639d2501c6c6fc40e4ed30535ba45\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "from dont_patronize_me import DontPatronizeMe as dpm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from DeepMoji.deepmoji.attlayer import AttentionWeightedAverage\n",
        "from DeepMoji import main_emoji as emoji\n",
        "from sklearn.utils import class_weight\n",
        "import pickle\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z14ORkuvHYyZ"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ATUFEz4hp87"
      },
      "source": [
        "## Load data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuNrIfLtvcQ5",
        "outputId": "af49b65a-1a59-458c-feca-75164e036226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map of label to numerical label:\n",
            "{'Unbalanced_power_relations': 0, 'Shallow_solution': 1, 'Presupposition': 2, 'Authority_voice': 3, 'Metaphors': 4, 'Compassion': 5, 'The_poorer_the_merrier': 6}\n"
          ]
        }
      ],
      "source": [
        "trn_file1 = \"dontpatronizeme_pcl.tsv\"\n",
        "trn_file2 = \"dontpatronizeme_categories.tsv\"\n",
        "tst_path = \"task4_test.tsv\"\n",
        "dpm_o = dpm('.', tst_path)\n",
        "dpm_o.load_task1()\n",
        "dpm_o.load_task2()\n",
        "dpm_o.load_test()\n",
        "data1 = dpm_o.train_task1_df\n",
        "data2 = dpm_o.train_task2_df\n",
        "data_tst = dpm_o.test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgrGbErSyj86"
      },
      "outputs": [],
      "source": [
        "data1 = data1[data1[\"text\"] != \"\"] #remove the empty text row 8639 with label 0\n",
        "for i, s in enumerate(data1[\"text\"]):\n",
        "  if s==\"\":\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Vsey7XnCHvEE",
        "outputId": "1da357bb-838c-40d3-804f-836c02e49abe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-145841c1-7b53-46ef-8088-56f5d44f5634\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4988</th>\n",
              "      <td>in fact , romance by far out-produces and out-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3053</th>\n",
              "      <td>in the past , dorel has had moments of brillia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1984</th>\n",
              "      <td>a man who reached out to a community facebook ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3547</th>\n",
              "      <td>other westerners who met similar fate to foley...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4891</th>\n",
              "      <td>\"the home affairs department must explain why ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7193</th>\n",
              "      <td>morris , who is blind , made the call in the s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8842</th>\n",
              "      <td>earlier , submitting the report on the last mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6110</th>\n",
              "      <td>\"\"\" the broad part of officials and the masses...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5848</th>\n",
              "      <td>it seems some chinese immigrants come across a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10172</th>\n",
              "      <td>\"meanwhile \"\" throughout this island , the hig...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2094 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-145841c1-7b53-46ef-8088-56f5d44f5634')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-145841c1-7b53-46ef-8088-56f5d44f5634 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-145841c1-7b53-46ef-8088-56f5d44f5634');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text\n",
              "4988   in fact , romance by far out-produces and out-...\n",
              "3053   in the past , dorel has had moments of brillia...\n",
              "1984   a man who reached out to a community facebook ...\n",
              "3547   other westerners who met similar fate to foley...\n",
              "4891   \"the home affairs department must explain why ...\n",
              "...                                                  ...\n",
              "7193   morris , who is blind , made the call in the s...\n",
              "8842   earlier , submitting the report on the last mo...\n",
              "6110   \"\"\" the broad part of officials and the masses...\n",
              "5848   it seems some chinese immigrants come across a...\n",
              "10172  \"meanwhile \"\" throughout this island , the hig...\n",
              "\n",
              "[2094 rows x 1 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## get test set from the origninal dataset\n",
        "x1, y1 = data1[\"text\"], data1[\"label\"]\n",
        "_, tst_x1, _, tst_y1 = train_test_split(x1, y1, test_size=0.2, random_state=36)\n",
        "tst_txt_df = pd.DataFrame(tst_x1)\n",
        "tst_txt_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gl-Gl5XMaLg"
      },
      "outputs": [],
      "source": [
        "## get the training set from the augmented dataset\n",
        "trndf1 = pd.read_csv(\"ten-langs-regex-cleaned-binary-label.csv\", encoding=\"utf-8\")\n",
        "trndf1 = trndf1.dropna() #drop the nan sentence on 733 row\n",
        "#trndf1 = trndf1.sample(frac=1) # shuffle the augmented data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Fr1qxtXiNXWq",
        "outputId": "fc020788-943b-413b-d620-567f123b0eff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bacff646-22de-4944-899c-e8b3207222f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"uk independence party ( ukip ) leader nigel f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nasruddin had asked of the measures taken by t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u.s. border patrol agents walk fences on the m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at this point in labour 's leadership battle b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tina anderson , clerk of the aleutians east bo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15151</th>\n",
              "      <td>\"\" My daughter , who was a physiotherapist , w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15152</th>\n",
              "      <td>The Turkish ambassador to tanzania ali davutog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15153</th>\n",
              "      <td>Finally , there seems to be a glimmer of hope ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15154</th>\n",
              "      <td>\" The current government is dedicated to solvi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15155</th>\n",
              "      <td>New delhi : activists and doctors have hailed ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15155 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bacff646-22de-4944-899c-e8b3207222f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bacff646-22de-4944-899c-e8b3207222f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bacff646-22de-4944-899c-e8b3207222f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text\n",
              "0      \"uk independence party ( ukip ) leader nigel f...\n",
              "1      nasruddin had asked of the measures taken by t...\n",
              "2      u.s. border patrol agents walk fences on the m...\n",
              "3      at this point in labour 's leadership battle b...\n",
              "4      tina anderson , clerk of the aleutians east bo...\n",
              "...                                                  ...\n",
              "15151  \"\" My daughter , who was a physiotherapist , w...\n",
              "15152  The Turkish ambassador to tanzania ali davutog...\n",
              "15153  Finally , there seems to be a glimmer of hope ...\n",
              "15154  \" The current government is dedicated to solvi...\n",
              "15155  New delhi : activists and doctors have hailed ...\n",
              "\n",
              "[15155 rows x 1 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn_x1 = trndf1[\"text\"]\n",
        "trn_txt_df = trndf1[['text']]\n",
        "trn_y1 = trndf1[\"label\"]\n",
        "trn_txt_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLiBeP1AzrxU"
      },
      "source": [
        "## Preprocessing (clean and tokenize the data):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y88WmON5Sf5",
        "outputId": "77ee33cc-6e9b-403d-d5ad-b867d24afc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1uIRtAN2YHX"
      },
      "outputs": [],
      "source": [
        "def tok_vec(types_num, trn_x, tst_x, matrix=False):\n",
        "  stopws = set(stopwords.words(\"english\"))\n",
        "  trn_x = [\" \".join([word for word in s.split() if word not in stopws]) for s in trn_x]\n",
        "  tst_x = [\" \".join([word for word in s.split() if word not in stopws]) for s in tst_x]\n",
        "  tokenizer = Tokenizer(num_words= types_num, filters = '@[\\\\]^{|}~\\t\\n)')   #keep the punctuations\n",
        "  tokenizer.fit_on_texts(trn_x)\n",
        "  trn_x_encoded = tokenizer.texts_to_sequences(trn_x)\n",
        "  tst_x_encoded = tokenizer.texts_to_sequences(tst_x)\n",
        "  #re = [(k, v) for k, v in sorted(tokenizer.word_counts.items(), key=lambda item: item[1], reverse=True)]\n",
        "  max_len = int(np.max([len(t) for t in trn_x_encoded])/4)\n",
        "  trn_x_encoded = np.array(pad_sequences(trn_x_encoded, maxlen=max_len, padding=\"post\", truncating=\"post\"))\n",
        "  tst_x_encoded = np.array(pad_sequences(tst_x_encoded, maxlen=max_len, padding=\"post\", truncating=\"post\"))\n",
        "  return trn_x_encoded, tst_x_encoded, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRmNsbMAGT2Q",
        "outputId": "a54d3b17-ec99-4f31-905b-b94938559b46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((15155, 124), (2094, 124), 124)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "types_num = 10000\n",
        "trn_x_encoded1, tst_x_encoded1, max_len = tok_vec(types_num, trn_x1, tst_x1)\n",
        "trn_x_encoded1.shape, tst_x_encoded1.shape, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WNl0O9JL5aN",
        "outputId": "989874b0-9558-4356-ffe3-0f53e59b1e96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((15155,), array([0, 0, 0, ..., 0, 0, 1]), 7569)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn_y_encoded1 = np.array(trn_y1)\n",
        "tst_y_encoded1 = np.array(tst_y1)\n",
        "trn_y_encoded1.shape, tst_y_encoded1, np.sum([l for l in trn_y_encoded1 if l==1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSNVrBBpsWLu",
        "outputId": "1273b4bf-10a3-4a10-dac6-96c3db205e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({0: 0.9988795148958608, 1: 1.001123001717532}, array([0, 1]))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the class labels are far from balanced, so we use class_weight\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = dict(zip(np.unique(trn_y_encoded1), class_weight.compute_class_weight(class_weight='balanced', \n",
        "                                                  classes=np.unique(trn_y_encoded1), y=trn_y_encoded1)))\n",
        "class_weights, np.unique(trn_y_encoded1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-KP9u1gM4wB"
      },
      "source": [
        "### Get sentiment scoring features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8gimZFytNg0N",
        "outputId": "6f8c3578-111c-4ba5-fd54-6edce0bda9bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting flair\n",
            "  Downloading flair-0.10-py3-none-any.whl (322 kB)\n",
            "\u001b[K     |████████████████████████████████| 322 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.62.3)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.2)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.10.0+cu111)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 52.0 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.0.0\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.13.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.10.0.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.10.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (3.0.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, sqlitedict, ftfy, langdetect, wikipedia-api\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=f9e0d59cb344b328eb0a4eb9bdd9972172255de8719e1295dbd2450b3670ad97\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=375afb10a3a033b4302c4c7c0d8bc7cc80ac369e5727cd6d53a8615167999404\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=f578e8b5637e53872cb57cb582c20bc85c6f36a04618e7b3de042b886a60e3b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14393 sha256=038765766401843c5f801b6ff7bbd21c39b14d7839a2f1bfbacd053e731fb188\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=0d36a018de129d5485825f3ec9dbc2143b2992e78ea81a75971751dcddbc4b52\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=46ce2b2df77c663caca147b3168dd2ee54f1bb9f38612e61d46eccd0c5b6885a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=8f4338d09d35ac106a0e027e7f798f83e2c0b2154be2fba1a33a6d149a1db4cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built gdown mpld3 overrides sqlitedict ftfy langdetect wikipedia-api\n",
            "Installing collected packages: requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, mpld3, more-itertools, langdetect, konoha, janome, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.10.1\n",
            "    Uninstalling importlib-metadata-4.10.1:\n",
            "      Successfully uninstalled importlib-metadata-4.10.1\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.0\n",
            "    Uninstalling gdown-4.2.0:\n",
            "      Successfully uninstalled gdown-4.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.10 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.4.0 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pyyaml-6.0 requests-2.27.1 sacremoses-0.0.47 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.11.4 transformers-4.16.2 wikipedia-api-0.5.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([[ 0.    ,  0.878 ,  0.122 ,  0.802 ],\n",
              "       [ 0.179 ,  0.776 ,  0.044 , -0.8169],\n",
              "       [ 0.072 ,  0.722 ,  0.206 ,  0.6128],\n",
              "       ...,\n",
              "       [ 0.    ,  0.88  ,  0.12  ,  0.836 ],\n",
              "       [ 0.05  ,  0.777 ,  0.174 ,  0.6108],\n",
              "       [ 0.27  ,  0.671 ,  0.059 , -0.886 ]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install vaderSentiment\n",
        "!pip install flair\n",
        "from sentiment_scoring import senti_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "trn_x_sc1 = np.array(senti_score(trn_x1, \"vader\"))\n",
        "tst_x_sc1 = np.array(senti_score(tst_x1, \"vader\"))\n",
        "tst_x_sc1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmbf66AmNEjE"
      },
      "source": [
        "### Stack sentiment scores to data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsM_FX1rNKOx",
        "outputId": "fe5361df-9391-491d-a270-08ffe8d802b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((2094, 128), 128)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn_x_encoded_with_senti = np.hstack((trn_x_encoded1, trn_x_sc1))\n",
        "tst_x_encoded_with_senti = np.hstack((tst_x_encoded1, tst_x_sc1))\n",
        "max_len = max_len + trn_x_sc1.shape[1]\n",
        "tst_x_encoded_with_senti.shape, max_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_pkySViya-p"
      },
      "source": [
        "### Get predictions from emoji model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vIlEetD1yizc",
        "outputId": "1610133d-50b8-4a1f-bbb6-b12b99cbd09a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing using dictionary from /content/drive/MyDrive/PCL/DeepMoji/model/vocabulary.json\n",
            "Loading model from /content/drive/MyDrive/PCL/DeepMoji/model/deepmoji_weights.hdf5.\n",
            "Model: \"DeepMoji\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 128, 256)     12800000    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 128, 256)     0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " bi_lstm_0 (Bidirectional)      (None, 128, 1024)    3149824     ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " bi_lstm_1 (Bidirectional)      (None, 128, 1024)    6295552     ['bi_lstm_0[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 128, 2304)    0           ['bi_lstm_1[0][0]',              \n",
            "                                                                  'bi_lstm_0[0][0]',              \n",
            "                                                                  'activation[0][0]']             \n",
            "                                                                                                  \n",
            " attlayer (AttentionWeightedAve  (None, 2304)        2304        ['concatenate[0][0]']            \n",
            " rage)                                                                                            \n",
            "                                                                                                  \n",
            " softmax (Dense)                (None, 64)           147520      ['attlayer[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 22,395,200\n",
            "Trainable params: 22,395,200\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Running predictions.\n",
            "Prediction for 1st sentence: [0.00957415 0.00885511 0.00196478 0.00486136 0.0033093  0.00720232\n",
            " 0.01765698 0.01540922 0.01702485 0.00639228 0.00774321 0.01916116\n",
            " 0.00844318 0.0235884  0.00300359 0.00552495 0.01589662 0.0311763\n",
            " 0.00442181 0.01469264 0.01021684 0.05179857 0.01683663 0.00316991\n",
            " 0.03483111 0.02589834 0.00263375 0.01275012 0.00778916 0.00300929\n",
            " 0.0268252  0.01630931 0.0175912  0.1013795  0.01880639 0.00739894\n",
            " 0.00370506 0.00820258 0.00591774 0.00440951 0.0524019  0.02858493\n",
            " 0.01012624 0.00665929 0.0081509  0.01346494 0.01795573 0.02430992\n",
            " 0.00552129 0.00570478 0.0260033  0.00616055 0.00743594 0.02578369\n",
            " 0.01978095 0.01550773 0.00416568 0.01625954 0.02392259 0.01062668\n",
            " 0.00618385 0.03114094 0.01336865 0.01539862]\n",
            "[['\"uk independence party ( ukip ) leader nigel farage tweeted : \"\" mass immigration still hopelessly out of control and set to get worse if we remain in eu . \"\"\"', 0.2715873774141073, 33, 40, 21, 24, 17, 0.1013795, 0.052401897, 0.05179857, 0.034831107, 0.031176304, 0.00957415, 0.008855115, 0.0019647751, 0.0048613604, 0.0033093032, 0.0072023175, 0.017656978, 0.015409224, 0.01702485, 0.006392283, 0.0077432133, 0.019161155, 0.008443182, 0.0235884, 0.003003586, 0.0055249487, 0.015896618, 0.031176304, 0.0044218074, 0.01469264, 0.010216839, 0.05179857, 0.01683663, 0.0031699075, 0.034831107, 0.025898337, 0.002633754, 0.012750124, 0.007789162, 0.0030092946, 0.026825197, 0.016309313, 0.017591204, 0.1013795, 0.01880639, 0.0073989364, 0.0037050566, 0.008202578, 0.0059177377, 0.0044095092, 0.052401897, 0.028584935, 0.010126236, 0.0066592917, 0.008150901, 0.013464942, 0.017955732, 0.02430992, 0.005521288, 0.0057047782, 0.026003305, 0.006160555, 0.0074359397, 0.025783692, 0.019780954, 0.015507728, 0.0041656755, 0.01625954, 0.02392259, 0.010626681, 0.00618385, 0.03114094, 0.013368652, 0.015398616], ['nasruddin had asked of the measures taken by the government to stop the influx of illegal immigrants in the country .', 0.38619766384363174, 32, 55, 25, 19, 12, 0.11460229, 0.08023287, 0.07215419, 0.06957487, 0.049633443, 0.03274294, 0.0304852, 0.006344109, 0.010073997, 0.0005536519, 0.009101882, 0.0055169687, 0.004149788, 0.00154592, 0.0053444398, 0.008793692, 0.0013663281, 0.049633443, 0.004239261, 0.006813403, 0.0018170773, 0.0022670056, 0.0040647686, 0.00045803483, 0.06957487, 0.020581404, 0.005314987, 0.038028352, 0.00036711537, 0.00054919673, 0.07215419, 0.0029779526, 0.014882047, 0.018154126, 0.008184921, 0.0055488404, 0.006741392, 0.11460229, 0.019335698, 0.01684126, 0.008539451, 0.0011855292, 0.03378096, 0.015510377, 0.012618451, 0.013624442, 0.014517057, 0.022599021, 0.014966112, 0.011915405, 0.015101528, 0.008409732, 0.0014181344, 0.0004795609, 0.009700466, 0.0054366714, 0.030763628, 0.016011473, 0.011667114, 0.0070054485, 0.08023287, 0.010447711, 0.004546621, 0.016290424, 0.0012946978, 0.00046506838, 0.003092482, 0.028191907, 0.0010370595], ['u.s. border patrol agents walk fences on the mexican border , and detain illegal immigrants - including children - before returning them to south of the border . a majority will try to cross the border again', 0.380889929831028, 44, 32, 55, 33, 25, 0.11054165, 0.10362552, 0.07016549, 0.058596395, 0.037960872, 0.006054012, 0.009758406, 0.0018002171, 0.0016949386, 0.00084402127, 0.0036782026, 0.0063536903, 0.014457453, 0.0056697614, 0.003398674, 0.007277431, 0.006870519, 0.01922107, 0.009907151, 0.0015267918, 0.0022252612, 0.011050662, 0.0055816923, 0.0013782434, 0.024029275, 0.013829818, 0.020752618, 0.033680666, 0.0012399383, 0.011153304, 0.037960872, 0.001962619, 0.01118654, 0.0052027325, 0.0024158047, 0.019259725, 0.027774626, 0.10362552, 0.058596395, 0.011709219, 0.0028072, 0.005045419, 0.020905368, 0.0046366784, 0.0076572224, 0.012628204, 0.019146785, 0.012809614, 0.01279856, 0.11054165, 0.0103521105, 0.0049491907, 0.009861, 0.0014062313, 0.0027976632, 0.026326882, 0.0096468935, 0.01525858, 0.029581219, 0.03693165, 0.07016549, 0.007772462, 0.0058857654, 0.011287967, 0.0045644473, 0.0044564675, 0.0044833776, 0.025940556, 0.0062274067]]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bfd08766-a87b-4bfa-b090-456013a6afb3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"uk independence party ( ukip ) leader nigel f...</td>\n",
              "      <td>0.271587</td>\n",
              "      <td>33</td>\n",
              "      <td>40</td>\n",
              "      <td>21</td>\n",
              "      <td>24</td>\n",
              "      <td>17</td>\n",
              "      <td>0.101379</td>\n",
              "      <td>0.052402</td>\n",
              "      <td>0.051799</td>\n",
              "      <td>0.034831</td>\n",
              "      <td>0.031176</td>\n",
              "      <td>0.009574</td>\n",
              "      <td>0.008855</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.004861</td>\n",
              "      <td>0.003309</td>\n",
              "      <td>0.007202</td>\n",
              "      <td>0.017657</td>\n",
              "      <td>0.015409</td>\n",
              "      <td>0.017025</td>\n",
              "      <td>0.006392</td>\n",
              "      <td>0.007743</td>\n",
              "      <td>0.019161</td>\n",
              "      <td>0.008443</td>\n",
              "      <td>0.023588</td>\n",
              "      <td>0.003004</td>\n",
              "      <td>0.005525</td>\n",
              "      <td>0.015897</td>\n",
              "      <td>0.031176</td>\n",
              "      <td>0.004422</td>\n",
              "      <td>0.014693</td>\n",
              "      <td>0.010217</td>\n",
              "      <td>0.051799</td>\n",
              "      <td>0.016837</td>\n",
              "      <td>0.003170</td>\n",
              "      <td>0.034831</td>\n",
              "      <td>0.025898</td>\n",
              "      <td>0.002634</td>\n",
              "      <td>0.012750</td>\n",
              "      <td>0.007789</td>\n",
              "      <td>0.003009</td>\n",
              "      <td>0.026825</td>\n",
              "      <td>0.016309</td>\n",
              "      <td>0.017591</td>\n",
              "      <td>0.101379</td>\n",
              "      <td>0.018806</td>\n",
              "      <td>0.007399</td>\n",
              "      <td>0.003705</td>\n",
              "      <td>0.008203</td>\n",
              "      <td>0.005918</td>\n",
              "      <td>0.004410</td>\n",
              "      <td>0.052402</td>\n",
              "      <td>0.028585</td>\n",
              "      <td>0.010126</td>\n",
              "      <td>0.006659</td>\n",
              "      <td>0.008151</td>\n",
              "      <td>0.013465</td>\n",
              "      <td>0.017956</td>\n",
              "      <td>0.024310</td>\n",
              "      <td>0.005521</td>\n",
              "      <td>0.005705</td>\n",
              "      <td>0.026003</td>\n",
              "      <td>0.006161</td>\n",
              "      <td>0.007436</td>\n",
              "      <td>0.025784</td>\n",
              "      <td>0.019781</td>\n",
              "      <td>0.015508</td>\n",
              "      <td>0.004166</td>\n",
              "      <td>0.016260</td>\n",
              "      <td>0.023923</td>\n",
              "      <td>0.010627</td>\n",
              "      <td>0.006184</td>\n",
              "      <td>0.031141</td>\n",
              "      <td>0.013369</td>\n",
              "      <td>0.015399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nasruddin had asked of the measures taken by t...</td>\n",
              "      <td>0.386198</td>\n",
              "      <td>32</td>\n",
              "      <td>55</td>\n",
              "      <td>25</td>\n",
              "      <td>19</td>\n",
              "      <td>12</td>\n",
              "      <td>0.114602</td>\n",
              "      <td>0.080233</td>\n",
              "      <td>0.072154</td>\n",
              "      <td>0.069575</td>\n",
              "      <td>0.049633</td>\n",
              "      <td>0.032743</td>\n",
              "      <td>0.030485</td>\n",
              "      <td>0.006344</td>\n",
              "      <td>0.010074</td>\n",
              "      <td>0.000554</td>\n",
              "      <td>0.009102</td>\n",
              "      <td>0.005517</td>\n",
              "      <td>0.004150</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.005344</td>\n",
              "      <td>0.008794</td>\n",
              "      <td>0.001366</td>\n",
              "      <td>0.049633</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>0.006813</td>\n",
              "      <td>0.001817</td>\n",
              "      <td>0.002267</td>\n",
              "      <td>0.004065</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.069575</td>\n",
              "      <td>0.020581</td>\n",
              "      <td>0.005315</td>\n",
              "      <td>0.038028</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.072154</td>\n",
              "      <td>0.002978</td>\n",
              "      <td>0.014882</td>\n",
              "      <td>0.018154</td>\n",
              "      <td>0.008185</td>\n",
              "      <td>0.005549</td>\n",
              "      <td>0.006741</td>\n",
              "      <td>0.114602</td>\n",
              "      <td>0.019336</td>\n",
              "      <td>0.016841</td>\n",
              "      <td>0.008539</td>\n",
              "      <td>0.001186</td>\n",
              "      <td>0.033781</td>\n",
              "      <td>0.015510</td>\n",
              "      <td>0.012618</td>\n",
              "      <td>0.013624</td>\n",
              "      <td>0.014517</td>\n",
              "      <td>0.022599</td>\n",
              "      <td>0.014966</td>\n",
              "      <td>0.011915</td>\n",
              "      <td>0.015102</td>\n",
              "      <td>0.008410</td>\n",
              "      <td>0.001418</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>0.009700</td>\n",
              "      <td>0.005437</td>\n",
              "      <td>0.030764</td>\n",
              "      <td>0.016011</td>\n",
              "      <td>0.011667</td>\n",
              "      <td>0.007005</td>\n",
              "      <td>0.080233</td>\n",
              "      <td>0.010448</td>\n",
              "      <td>0.004547</td>\n",
              "      <td>0.016290</td>\n",
              "      <td>0.001295</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.003092</td>\n",
              "      <td>0.028192</td>\n",
              "      <td>0.001037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u.s. border patrol agents walk fences on the m...</td>\n",
              "      <td>0.380890</td>\n",
              "      <td>44</td>\n",
              "      <td>32</td>\n",
              "      <td>55</td>\n",
              "      <td>33</td>\n",
              "      <td>25</td>\n",
              "      <td>0.110542</td>\n",
              "      <td>0.103626</td>\n",
              "      <td>0.070165</td>\n",
              "      <td>0.058596</td>\n",
              "      <td>0.037961</td>\n",
              "      <td>0.006054</td>\n",
              "      <td>0.009758</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.001695</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.003678</td>\n",
              "      <td>0.006354</td>\n",
              "      <td>0.014457</td>\n",
              "      <td>0.005670</td>\n",
              "      <td>0.003399</td>\n",
              "      <td>0.007277</td>\n",
              "      <td>0.006871</td>\n",
              "      <td>0.019221</td>\n",
              "      <td>0.009907</td>\n",
              "      <td>0.001527</td>\n",
              "      <td>0.002225</td>\n",
              "      <td>0.011051</td>\n",
              "      <td>0.005582</td>\n",
              "      <td>0.001378</td>\n",
              "      <td>0.024029</td>\n",
              "      <td>0.013830</td>\n",
              "      <td>0.020753</td>\n",
              "      <td>0.033681</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.011153</td>\n",
              "      <td>0.037961</td>\n",
              "      <td>0.001963</td>\n",
              "      <td>0.011187</td>\n",
              "      <td>0.005203</td>\n",
              "      <td>0.002416</td>\n",
              "      <td>0.019260</td>\n",
              "      <td>0.027775</td>\n",
              "      <td>0.103626</td>\n",
              "      <td>0.058596</td>\n",
              "      <td>0.011709</td>\n",
              "      <td>0.002807</td>\n",
              "      <td>0.005045</td>\n",
              "      <td>0.020905</td>\n",
              "      <td>0.004637</td>\n",
              "      <td>0.007657</td>\n",
              "      <td>0.012628</td>\n",
              "      <td>0.019147</td>\n",
              "      <td>0.012810</td>\n",
              "      <td>0.012799</td>\n",
              "      <td>0.110542</td>\n",
              "      <td>0.010352</td>\n",
              "      <td>0.004949</td>\n",
              "      <td>0.009861</td>\n",
              "      <td>0.001406</td>\n",
              "      <td>0.002798</td>\n",
              "      <td>0.026327</td>\n",
              "      <td>0.009647</td>\n",
              "      <td>0.015259</td>\n",
              "      <td>0.029581</td>\n",
              "      <td>0.036932</td>\n",
              "      <td>0.070165</td>\n",
              "      <td>0.007772</td>\n",
              "      <td>0.005886</td>\n",
              "      <td>0.011288</td>\n",
              "      <td>0.004564</td>\n",
              "      <td>0.004456</td>\n",
              "      <td>0.004483</td>\n",
              "      <td>0.025941</td>\n",
              "      <td>0.006227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at this point in labour 's leadership battle b...</td>\n",
              "      <td>0.488369</td>\n",
              "      <td>44</td>\n",
              "      <td>32</td>\n",
              "      <td>55</td>\n",
              "      <td>33</td>\n",
              "      <td>22</td>\n",
              "      <td>0.157494</td>\n",
              "      <td>0.144506</td>\n",
              "      <td>0.086157</td>\n",
              "      <td>0.050563</td>\n",
              "      <td>0.049649</td>\n",
              "      <td>0.002976</td>\n",
              "      <td>0.007366</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.000473</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.004099</td>\n",
              "      <td>0.004310</td>\n",
              "      <td>0.007518</td>\n",
              "      <td>0.004589</td>\n",
              "      <td>0.002291</td>\n",
              "      <td>0.004177</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.014764</td>\n",
              "      <td>0.009265</td>\n",
              "      <td>0.004968</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.004805</td>\n",
              "      <td>0.004067</td>\n",
              "      <td>0.000623</td>\n",
              "      <td>0.017007</td>\n",
              "      <td>0.007788</td>\n",
              "      <td>0.024762</td>\n",
              "      <td>0.049649</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.008319</td>\n",
              "      <td>0.034932</td>\n",
              "      <td>0.000787</td>\n",
              "      <td>0.012882</td>\n",
              "      <td>0.003286</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.013474</td>\n",
              "      <td>0.008884</td>\n",
              "      <td>0.144506</td>\n",
              "      <td>0.050563</td>\n",
              "      <td>0.012760</td>\n",
              "      <td>0.004979</td>\n",
              "      <td>0.000920</td>\n",
              "      <td>0.024595</td>\n",
              "      <td>0.004142</td>\n",
              "      <td>0.003030</td>\n",
              "      <td>0.014078</td>\n",
              "      <td>0.047849</td>\n",
              "      <td>0.004245</td>\n",
              "      <td>0.009064</td>\n",
              "      <td>0.157494</td>\n",
              "      <td>0.009242</td>\n",
              "      <td>0.009085</td>\n",
              "      <td>0.009701</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.015653</td>\n",
              "      <td>0.004296</td>\n",
              "      <td>0.008730</td>\n",
              "      <td>0.014646</td>\n",
              "      <td>0.012520</td>\n",
              "      <td>0.086157</td>\n",
              "      <td>0.005241</td>\n",
              "      <td>0.011222</td>\n",
              "      <td>0.033331</td>\n",
              "      <td>0.005598</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.008938</td>\n",
              "      <td>0.016112</td>\n",
              "      <td>0.004652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tina anderson , clerk of the aleutians east bo...</td>\n",
              "      <td>0.316807</td>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "      <td>55</td>\n",
              "      <td>21</td>\n",
              "      <td>61</td>\n",
              "      <td>0.102034</td>\n",
              "      <td>0.071767</td>\n",
              "      <td>0.060440</td>\n",
              "      <td>0.043257</td>\n",
              "      <td>0.039309</td>\n",
              "      <td>0.005167</td>\n",
              "      <td>0.008380</td>\n",
              "      <td>0.001461</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.006755</td>\n",
              "      <td>0.008682</td>\n",
              "      <td>0.007333</td>\n",
              "      <td>0.024936</td>\n",
              "      <td>0.002021</td>\n",
              "      <td>0.004072</td>\n",
              "      <td>0.018761</td>\n",
              "      <td>0.021065</td>\n",
              "      <td>0.011614</td>\n",
              "      <td>0.001764</td>\n",
              "      <td>0.001736</td>\n",
              "      <td>0.005401</td>\n",
              "      <td>0.012058</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>0.019880</td>\n",
              "      <td>0.006540</td>\n",
              "      <td>0.043257</td>\n",
              "      <td>0.033436</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.023925</td>\n",
              "      <td>0.034852</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.015497</td>\n",
              "      <td>0.003324</td>\n",
              "      <td>0.001699</td>\n",
              "      <td>0.019145</td>\n",
              "      <td>0.019721</td>\n",
              "      <td>0.102034</td>\n",
              "      <td>0.071767</td>\n",
              "      <td>0.027251</td>\n",
              "      <td>0.004672</td>\n",
              "      <td>0.001286</td>\n",
              "      <td>0.015483</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>0.005372</td>\n",
              "      <td>0.037268</td>\n",
              "      <td>0.015475</td>\n",
              "      <td>0.010001</td>\n",
              "      <td>0.009111</td>\n",
              "      <td>0.027143</td>\n",
              "      <td>0.009656</td>\n",
              "      <td>0.017652</td>\n",
              "      <td>0.027962</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>0.001799</td>\n",
              "      <td>0.011567</td>\n",
              "      <td>0.008130</td>\n",
              "      <td>0.010631</td>\n",
              "      <td>0.017657</td>\n",
              "      <td>0.015933</td>\n",
              "      <td>0.060440</td>\n",
              "      <td>0.002172</td>\n",
              "      <td>0.007501</td>\n",
              "      <td>0.019262</td>\n",
              "      <td>0.016522</td>\n",
              "      <td>0.007316</td>\n",
              "      <td>0.039309</td>\n",
              "      <td>0.015503</td>\n",
              "      <td>0.006915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15150</th>\n",
              "      <td>\"\" My daughter , who was a physiotherapist , w...</td>\n",
              "      <td>0.285413</td>\n",
              "      <td>13</td>\n",
              "      <td>33</td>\n",
              "      <td>24</td>\n",
              "      <td>40</td>\n",
              "      <td>21</td>\n",
              "      <td>0.073691</td>\n",
              "      <td>0.072557</td>\n",
              "      <td>0.063297</td>\n",
              "      <td>0.040068</td>\n",
              "      <td>0.035800</td>\n",
              "      <td>0.003565</td>\n",
              "      <td>0.010571</td>\n",
              "      <td>0.002154</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>0.002170</td>\n",
              "      <td>0.007051</td>\n",
              "      <td>0.018301</td>\n",
              "      <td>0.013934</td>\n",
              "      <td>0.025421</td>\n",
              "      <td>0.005099</td>\n",
              "      <td>0.004651</td>\n",
              "      <td>0.002477</td>\n",
              "      <td>0.010249</td>\n",
              "      <td>0.073691</td>\n",
              "      <td>0.002408</td>\n",
              "      <td>0.006830</td>\n",
              "      <td>0.013776</td>\n",
              "      <td>0.025664</td>\n",
              "      <td>0.008370</td>\n",
              "      <td>0.014293</td>\n",
              "      <td>0.005363</td>\n",
              "      <td>0.035800</td>\n",
              "      <td>0.015810</td>\n",
              "      <td>0.005496</td>\n",
              "      <td>0.063297</td>\n",
              "      <td>0.022013</td>\n",
              "      <td>0.006465</td>\n",
              "      <td>0.010303</td>\n",
              "      <td>0.004352</td>\n",
              "      <td>0.002194</td>\n",
              "      <td>0.014752</td>\n",
              "      <td>0.008336</td>\n",
              "      <td>0.033267</td>\n",
              "      <td>0.072557</td>\n",
              "      <td>0.012324</td>\n",
              "      <td>0.005268</td>\n",
              "      <td>0.003273</td>\n",
              "      <td>0.015210</td>\n",
              "      <td>0.007578</td>\n",
              "      <td>0.002918</td>\n",
              "      <td>0.040068</td>\n",
              "      <td>0.027790</td>\n",
              "      <td>0.007693</td>\n",
              "      <td>0.006519</td>\n",
              "      <td>0.027083</td>\n",
              "      <td>0.007569</td>\n",
              "      <td>0.019766</td>\n",
              "      <td>0.033939</td>\n",
              "      <td>0.001224</td>\n",
              "      <td>0.004185</td>\n",
              "      <td>0.016605</td>\n",
              "      <td>0.007958</td>\n",
              "      <td>0.007010</td>\n",
              "      <td>0.013479</td>\n",
              "      <td>0.009133</td>\n",
              "      <td>0.025554</td>\n",
              "      <td>0.007985</td>\n",
              "      <td>0.008502</td>\n",
              "      <td>0.034643</td>\n",
              "      <td>0.023106</td>\n",
              "      <td>0.022482</td>\n",
              "      <td>0.022144</td>\n",
              "      <td>0.010498</td>\n",
              "      <td>0.014673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15151</th>\n",
              "      <td>The Turkish ambassador to tanzania ali davutog...</td>\n",
              "      <td>0.422840</td>\n",
              "      <td>32</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>33</td>\n",
              "      <td>25</td>\n",
              "      <td>0.170722</td>\n",
              "      <td>0.132489</td>\n",
              "      <td>0.042173</td>\n",
              "      <td>0.039675</td>\n",
              "      <td>0.037781</td>\n",
              "      <td>0.004538</td>\n",
              "      <td>0.016309</td>\n",
              "      <td>0.002802</td>\n",
              "      <td>0.003133</td>\n",
              "      <td>0.000402</td>\n",
              "      <td>0.011745</td>\n",
              "      <td>0.005045</td>\n",
              "      <td>0.006188</td>\n",
              "      <td>0.003988</td>\n",
              "      <td>0.002332</td>\n",
              "      <td>0.006048</td>\n",
              "      <td>0.001615</td>\n",
              "      <td>0.012622</td>\n",
              "      <td>0.007151</td>\n",
              "      <td>0.005046</td>\n",
              "      <td>0.002570</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>0.005999</td>\n",
              "      <td>0.001359</td>\n",
              "      <td>0.029053</td>\n",
              "      <td>0.009437</td>\n",
              "      <td>0.024623</td>\n",
              "      <td>0.042173</td>\n",
              "      <td>0.000788</td>\n",
              "      <td>0.003842</td>\n",
              "      <td>0.037781</td>\n",
              "      <td>0.001644</td>\n",
              "      <td>0.028830</td>\n",
              "      <td>0.006298</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.013074</td>\n",
              "      <td>0.006702</td>\n",
              "      <td>0.170722</td>\n",
              "      <td>0.039675</td>\n",
              "      <td>0.030252</td>\n",
              "      <td>0.011299</td>\n",
              "      <td>0.001257</td>\n",
              "      <td>0.036947</td>\n",
              "      <td>0.009568</td>\n",
              "      <td>0.005770</td>\n",
              "      <td>0.016789</td>\n",
              "      <td>0.012125</td>\n",
              "      <td>0.009198</td>\n",
              "      <td>0.021859</td>\n",
              "      <td>0.028728</td>\n",
              "      <td>0.020511</td>\n",
              "      <td>0.013905</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.002579</td>\n",
              "      <td>0.008211</td>\n",
              "      <td>0.005055</td>\n",
              "      <td>0.018600</td>\n",
              "      <td>0.011676</td>\n",
              "      <td>0.007488</td>\n",
              "      <td>0.132489</td>\n",
              "      <td>0.008371</td>\n",
              "      <td>0.005802</td>\n",
              "      <td>0.018978</td>\n",
              "      <td>0.003735</td>\n",
              "      <td>0.002622</td>\n",
              "      <td>0.004578</td>\n",
              "      <td>0.021495</td>\n",
              "      <td>0.003924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15152</th>\n",
              "      <td>Finally , there seems to be a glimmer of hope ...</td>\n",
              "      <td>0.445731</td>\n",
              "      <td>21</td>\n",
              "      <td>34</td>\n",
              "      <td>32</td>\n",
              "      <td>55</td>\n",
              "      <td>27</td>\n",
              "      <td>0.202241</td>\n",
              "      <td>0.076420</td>\n",
              "      <td>0.068175</td>\n",
              "      <td>0.053582</td>\n",
              "      <td>0.045313</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>0.005349</td>\n",
              "      <td>0.001551</td>\n",
              "      <td>0.001664</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.021553</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>0.011271</td>\n",
              "      <td>0.011849</td>\n",
              "      <td>0.000851</td>\n",
              "      <td>0.001910</td>\n",
              "      <td>0.003222</td>\n",
              "      <td>0.005473</td>\n",
              "      <td>0.006297</td>\n",
              "      <td>0.001575</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>0.010153</td>\n",
              "      <td>0.014914</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.008189</td>\n",
              "      <td>0.003065</td>\n",
              "      <td>0.202241</td>\n",
              "      <td>0.042094</td>\n",
              "      <td>0.001229</td>\n",
              "      <td>0.021407</td>\n",
              "      <td>0.016612</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.045313</td>\n",
              "      <td>0.001260</td>\n",
              "      <td>0.001887</td>\n",
              "      <td>0.012604</td>\n",
              "      <td>0.006196</td>\n",
              "      <td>0.068175</td>\n",
              "      <td>0.037066</td>\n",
              "      <td>0.076420</td>\n",
              "      <td>0.015370</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.014357</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.005812</td>\n",
              "      <td>0.017692</td>\n",
              "      <td>0.005295</td>\n",
              "      <td>0.003681</td>\n",
              "      <td>0.014186</td>\n",
              "      <td>0.039671</td>\n",
              "      <td>0.019843</td>\n",
              "      <td>0.041841</td>\n",
              "      <td>0.019617</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.006635</td>\n",
              "      <td>0.003173</td>\n",
              "      <td>0.014154</td>\n",
              "      <td>0.012748</td>\n",
              "      <td>0.005367</td>\n",
              "      <td>0.053582</td>\n",
              "      <td>0.001747</td>\n",
              "      <td>0.004541</td>\n",
              "      <td>0.006655</td>\n",
              "      <td>0.007807</td>\n",
              "      <td>0.006924</td>\n",
              "      <td>0.010968</td>\n",
              "      <td>0.007269</td>\n",
              "      <td>0.005314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15153</th>\n",
              "      <td>\" The current government is dedicated to solvi...</td>\n",
              "      <td>0.344221</td>\n",
              "      <td>33</td>\n",
              "      <td>21</td>\n",
              "      <td>40</td>\n",
              "      <td>17</td>\n",
              "      <td>13</td>\n",
              "      <td>0.107349</td>\n",
              "      <td>0.076197</td>\n",
              "      <td>0.062647</td>\n",
              "      <td>0.051639</td>\n",
              "      <td>0.046389</td>\n",
              "      <td>0.009645</td>\n",
              "      <td>0.008185</td>\n",
              "      <td>0.001791</td>\n",
              "      <td>0.003856</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.008614</td>\n",
              "      <td>0.027486</td>\n",
              "      <td>0.016161</td>\n",
              "      <td>0.008135</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.007889</td>\n",
              "      <td>0.009145</td>\n",
              "      <td>0.015302</td>\n",
              "      <td>0.046389</td>\n",
              "      <td>0.003627</td>\n",
              "      <td>0.007005</td>\n",
              "      <td>0.015393</td>\n",
              "      <td>0.051639</td>\n",
              "      <td>0.001937</td>\n",
              "      <td>0.017933</td>\n",
              "      <td>0.010047</td>\n",
              "      <td>0.076197</td>\n",
              "      <td>0.021009</td>\n",
              "      <td>0.001098</td>\n",
              "      <td>0.016828</td>\n",
              "      <td>0.029433</td>\n",
              "      <td>0.002983</td>\n",
              "      <td>0.012960</td>\n",
              "      <td>0.003680</td>\n",
              "      <td>0.002253</td>\n",
              "      <td>0.033881</td>\n",
              "      <td>0.017646</td>\n",
              "      <td>0.023351</td>\n",
              "      <td>0.107349</td>\n",
              "      <td>0.017122</td>\n",
              "      <td>0.006977</td>\n",
              "      <td>0.002023</td>\n",
              "      <td>0.010881</td>\n",
              "      <td>0.005817</td>\n",
              "      <td>0.002714</td>\n",
              "      <td>0.062647</td>\n",
              "      <td>0.016601</td>\n",
              "      <td>0.009858</td>\n",
              "      <td>0.006219</td>\n",
              "      <td>0.018542</td>\n",
              "      <td>0.009428</td>\n",
              "      <td>0.011227</td>\n",
              "      <td>0.012588</td>\n",
              "      <td>0.003567</td>\n",
              "      <td>0.004278</td>\n",
              "      <td>0.022490</td>\n",
              "      <td>0.010134</td>\n",
              "      <td>0.005839</td>\n",
              "      <td>0.032265</td>\n",
              "      <td>0.011792</td>\n",
              "      <td>0.019669</td>\n",
              "      <td>0.003074</td>\n",
              "      <td>0.011720</td>\n",
              "      <td>0.022380</td>\n",
              "      <td>0.004720</td>\n",
              "      <td>0.002747</td>\n",
              "      <td>0.006196</td>\n",
              "      <td>0.011873</td>\n",
              "      <td>0.010791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15154</th>\n",
              "      <td>New delhi : activists and doctors have hailed ...</td>\n",
              "      <td>0.439481</td>\n",
              "      <td>32</td>\n",
              "      <td>55</td>\n",
              "      <td>33</td>\n",
              "      <td>44</td>\n",
              "      <td>21</td>\n",
              "      <td>0.141514</td>\n",
              "      <td>0.111247</td>\n",
              "      <td>0.074378</td>\n",
              "      <td>0.061497</td>\n",
              "      <td>0.050847</td>\n",
              "      <td>0.001564</td>\n",
              "      <td>0.006049</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.001415</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.005868</td>\n",
              "      <td>0.006635</td>\n",
              "      <td>0.009481</td>\n",
              "      <td>0.011293</td>\n",
              "      <td>0.001474</td>\n",
              "      <td>0.004387</td>\n",
              "      <td>0.002296</td>\n",
              "      <td>0.006440</td>\n",
              "      <td>0.008185</td>\n",
              "      <td>0.000987</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>0.010970</td>\n",
              "      <td>0.010163</td>\n",
              "      <td>0.001748</td>\n",
              "      <td>0.011333</td>\n",
              "      <td>0.003804</td>\n",
              "      <td>0.050847</td>\n",
              "      <td>0.022586</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.035888</td>\n",
              "      <td>0.019198</td>\n",
              "      <td>0.000572</td>\n",
              "      <td>0.016699</td>\n",
              "      <td>0.002367</td>\n",
              "      <td>0.002157</td>\n",
              "      <td>0.017635</td>\n",
              "      <td>0.014278</td>\n",
              "      <td>0.141514</td>\n",
              "      <td>0.074378</td>\n",
              "      <td>0.029088</td>\n",
              "      <td>0.006922</td>\n",
              "      <td>0.001742</td>\n",
              "      <td>0.024444</td>\n",
              "      <td>0.001768</td>\n",
              "      <td>0.012578</td>\n",
              "      <td>0.019274</td>\n",
              "      <td>0.006467</td>\n",
              "      <td>0.010201</td>\n",
              "      <td>0.014349</td>\n",
              "      <td>0.061497</td>\n",
              "      <td>0.012941</td>\n",
              "      <td>0.020654</td>\n",
              "      <td>0.024628</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.001293</td>\n",
              "      <td>0.007927</td>\n",
              "      <td>0.005386</td>\n",
              "      <td>0.015838</td>\n",
              "      <td>0.017121</td>\n",
              "      <td>0.012219</td>\n",
              "      <td>0.111247</td>\n",
              "      <td>0.003159</td>\n",
              "      <td>0.008228</td>\n",
              "      <td>0.020220</td>\n",
              "      <td>0.007673</td>\n",
              "      <td>0.003787</td>\n",
              "      <td>0.009941</td>\n",
              "      <td>0.015365</td>\n",
              "      <td>0.005930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15155 rows × 76 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfd08766-a87b-4bfa-b090-456013a6afb3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bfd08766-a87b-4bfa-b090-456013a6afb3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bfd08766-a87b-4bfa-b090-456013a6afb3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                      0   ...        75\n",
              "0      \"uk independence party ( ukip ) leader nigel f...  ...  0.015399\n",
              "1      nasruddin had asked of the measures taken by t...  ...  0.001037\n",
              "2      u.s. border patrol agents walk fences on the m...  ...  0.006227\n",
              "3      at this point in labour 's leadership battle b...  ...  0.004652\n",
              "4      tina anderson , clerk of the aleutians east bo...  ...  0.006915\n",
              "...                                                  ...  ...       ...\n",
              "15150  \"\" My daughter , who was a physiotherapist , w...  ...  0.014673\n",
              "15151  The Turkish ambassador to tanzania ali davutog...  ...  0.003924\n",
              "15152  Finally , there seems to be a glimmer of hope ...  ...  0.005314\n",
              "15153  \" The current government is dedicated to solvi...  ...  0.010791\n",
              "15154  New delhi : activists and doctors have hailed ...  ...  0.005930\n",
              "\n",
              "[15155 rows x 76 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_emoji = emoji.emoji_prediction(trn_txt_df, maxlen = max_len)\n",
        "trn_x1_emoji = pd.DataFrame(pred_emoji)\n",
        "trn_x1_emoji "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOnD2QKs7ClA",
        "outputId": "821fbf81-6181-4973-ff28-02dad4b7f693"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15155, 64)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn_x1_emoji = trn_x1_emoji[trn_x1_emoji.columns[-64:]]\n",
        "np.array(trn_x1_emoji).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ILVS0pZQcsc",
        "outputId": "7c1d9440-77ee-401d-aac4-005e91c2d93e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing using dictionary from /content/drive/MyDrive/PCL/DeepMoji/model/vocabulary.json\n",
            "Loading model from /content/drive/MyDrive/PCL/DeepMoji/model/deepmoji_weights.hdf5.\n",
            "Model: \"DeepMoji\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 128, 256)     12800000    ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 128, 256)     0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " bi_lstm_0 (Bidirectional)      (None, 128, 1024)    3149824     ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " bi_lstm_1 (Bidirectional)      (None, 128, 1024)    6295552     ['bi_lstm_0[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 128, 2304)    0           ['bi_lstm_1[0][0]',              \n",
            "                                                                  'bi_lstm_0[0][0]',              \n",
            "                                                                  'activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " attlayer (AttentionWeightedAve  (None, 2304)        2304        ['concatenate_1[0][0]']          \n",
            " rage)                                                                                            \n",
            "                                                                                                  \n",
            " softmax (Dense)                (None, 64)           147520      ['attlayer[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 22,395,200\n",
            "Trainable params: 22,395,200\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Running predictions.\n",
            "Prediction for 1st sentence: [0.0013851  0.00189994 0.00043912 0.00044543 0.00457199 0.0010676\n",
            " 0.00998273 0.02251319 0.0525085  0.00282352 0.00390271 0.00389186\n",
            " 0.00443728 0.00963747 0.00081163 0.00444182 0.03767223 0.00801237\n",
            " 0.00901829 0.00348374 0.00313172 0.01635881 0.00794276 0.00595983\n",
            " 0.26143625 0.00767483 0.0029538  0.00265668 0.00136405 0.00039395\n",
            " 0.01969825 0.01566484 0.00619977 0.05150574 0.00310033 0.00117027\n",
            " 0.00281185 0.00270458 0.00186457 0.00171552 0.01729063 0.01006298\n",
            " 0.00365005 0.00198085 0.0243915  0.00221102 0.00422939 0.15261675\n",
            " 0.00133014 0.00119797 0.03391246 0.00266227 0.0025547  0.02434315\n",
            " 0.01676476 0.00560954 0.00170975 0.0043793  0.00824599 0.01862551\n",
            " 0.02521004 0.01617191 0.00638252 0.01521195]\n",
            "[['in fact , romance by far out-produces and out-sells all other genres of fiction . it is a broad category , with many subgenres that explore many different models for relationships between men and women . romance is read by women across many different demographics in age , education , class and race .', 0.5557394810020924, 24, 47, 8, 33, 16, 0.26143625, 0.15261675, 0.052508503, 0.051505737, 0.037672233, 0.0013850996, 0.0018999405, 0.00043912273, 0.00044542813, 0.004571993, 0.0010675992, 0.009982732, 0.022513192, 0.052508503, 0.0028235156, 0.0039027103, 0.0038918592, 0.0044372776, 0.009637469, 0.0008116264, 0.0044418206, 0.037672233, 0.008012371, 0.009018286, 0.0034837443, 0.0031317184, 0.016358808, 0.007942762, 0.005959828, 0.26143625, 0.0076748324, 0.0029537957, 0.0026566775, 0.0013640483, 0.00039394834, 0.019698251, 0.015664842, 0.006199768, 0.051505737, 0.0031003286, 0.0011702738, 0.0028118452, 0.002704579, 0.0018645682, 0.0017155155, 0.017290633, 0.01006298, 0.0036500485, 0.0019808495, 0.024391504, 0.0022110231, 0.0042293943, 0.15261675, 0.0013301383, 0.0011979713, 0.03391246, 0.0026622675, 0.0025546984, 0.02434315, 0.016764762, 0.005609541, 0.00170975, 0.0043793, 0.008245992, 0.01862551, 0.025210038, 0.01617191, 0.006382519, 0.015211945], [\"in the past , dorel has had moments of brilliance -- up 29% in 2016 and 44% in 2012 -- but the rest of the time it 's been hopeless , seriously underperforming the tsx , which has n't exactly warranted positive press in recent years .\", 0.2589873895049095, 32, 33, 22, 55, 34, 0.068947695, 0.058219463, 0.0500977, 0.04771989, 0.034002643, 0.003245696, 0.008785852, 0.0028434906, 0.0026612415, 0.0024802843, 0.014966127, 0.008486554, 0.012942493, 0.013403052, 0.0036051874, 0.0069530946, 0.0063616787, 0.018140877, 0.011973989, 0.0032202997, 0.0044203354, 0.012164329, 0.01140719, 0.003530908, 0.016137116, 0.007956585, 0.033052642, 0.0500977, 0.002430174, 0.018599307, 0.030214986, 0.0016065291, 0.03383224, 0.006142564, 0.0044740518, 0.019937998, 0.015614742, 0.068947695, 0.058219463, 0.034002643, 0.009218478, 0.0028001345, 0.017912187, 0.0042223507, 0.0068636085, 0.02696317, 0.013126158, 0.006454534, 0.022453267, 0.02418738, 0.0192487, 0.027263695, 0.018348679, 0.0023674949, 0.0034733529, 0.020719437, 0.0042987918, 0.021549623, 0.018552156, 0.016889088, 0.04771989, 0.0046814047, 0.020395717, 0.02050134, 0.010389561, 0.0072161495, 0.013242494, 0.020474652, 0.015607334], ['a man who reached out to a community facebook page to help his disabled brother find work said he could not be more grateful for the positive response they received .', 0.5574915632605553, 21, 33, 40, 17, 47, 0.26955724, 0.09528754, 0.0705573, 0.0650045, 0.057084993, 0.00062527036, 0.0004621235, 0.00040330354, 0.0008492112, 0.0023993787, 0.0026532067, 0.014726648, 0.027974932, 0.052416414, 0.0005903011, 0.0031697452, 0.0006721165, 0.0022729516, 0.038403526, 0.0007294155, 0.0075311875, 0.018633367, 0.0650045, 0.011133904, 0.000596706, 0.0013327472, 0.26955724, 0.0027378881, 0.003627289, 0.03011968, 0.0016593087, 0.00052260933, 0.0032738526, 0.0008616059, 0.00030261025, 0.013187802, 0.0039386037, 0.0024183586, 0.09528754, 0.007893097, 0.0024616367, 0.00072301755, 0.0013229958, 0.0012242199, 0.00066622614, 0.0705573, 0.00447995, 0.00046374995, 0.0009480965, 0.0017973592, 0.0020695657, 0.0040067043, 0.057084993, 0.00027790377, 0.00071939355, 0.0073429844, 0.0007281929, 0.00087641, 0.013701257, 0.0013672682, 0.0018313578, 0.0010978809, 0.020555008, 0.015019136, 0.023979317, 0.01642025, 0.03545778, 0.002671136, 0.022180492]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2094, 64)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tst_x1_emoji = pd.DataFrame(emoji.emoji_prediction(tst_txt_df, maxlen=max_len))\n",
        "tst_x1_emoji = tst_x1_emoji[tst_x1_emoji.columns[-64:]]\n",
        "tst_x1_emoji.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWcSLUnH5ncG"
      },
      "source": [
        "### Get stacked models and their predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11jUIwFq6bYd"
      },
      "outputs": [],
      "source": [
        "#!pip install pygraphviz\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Input, concatenate, Bidirectional, Embedding, Dropout, Activation, Attention\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDJR_87p5qGs",
        "outputId": "ebac1d45-59af-4850-a6e0-a9e608453884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Model1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 124)]        0           []                               \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " concat_input (Concatenate)     (None, 128)          0           ['input1[0][0]',                 \n",
            "                                                                  'input2[0][0]']                 \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 128, 128)     1280512     ['concat_input[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 128, 128)     0           ['embedding_5[0][0]']            \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 128, 128)     0           ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " BiLSTM0 (Bidirectional)        (None, 128)          98816       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " Dense_layer (Dense)            (None, 32)           4128        ['BiLSTM0[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 32)           0           ['Dense_layer[0][0]']            \n",
            "                                                                                                  \n",
            " Output (Dense)                 (None, 1)            33          ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,383,489\n",
            "Trainable params: 1,383,489\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "379/379 - 124s - loss: 0.2817 - accuracy: 0.8801 - val_loss: 0.0638 - val_accuracy: 0.9795 - 124s/epoch - 328ms/step\n",
            "Epoch 2/100\n",
            "379/379 - 96s - loss: 0.0670 - accuracy: 0.9786 - val_loss: 0.1137 - val_accuracy: 0.9555 - 96s/epoch - 253ms/step\n",
            "Epoch 3/100\n",
            "379/379 - 95s - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.0586 - val_accuracy: 0.9828 - 95s/epoch - 250ms/step\n",
            "Epoch 4/100\n",
            "379/379 - 93s - loss: 0.0109 - accuracy: 0.9966 - val_loss: 0.0900 - val_accuracy: 0.9756 - 93s/epoch - 246ms/step\n",
            "Epoch 5/100\n",
            "379/379 - 93s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0378 - val_accuracy: 0.9885 - 93s/epoch - 247ms/step\n",
            "Epoch 6/100\n",
            "379/379 - 94s - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.1313 - val_accuracy: 0.9677 - 94s/epoch - 247ms/step\n",
            "Epoch 7/100\n",
            "379/379 - 93s - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0575 - val_accuracy: 0.9865 - 93s/epoch - 247ms/step\n",
            "Epoch 8/100\n",
            "379/379 - 94s - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0712 - val_accuracy: 0.9861 - 94s/epoch - 247ms/step\n",
            "Epoch 9/100\n",
            "379/379 - 93s - loss: 0.0028 - accuracy: 0.9989 - val_loss: 0.1261 - val_accuracy: 0.9723 - 93s/epoch - 247ms/step\n",
            "Epoch 10/100\n",
            "379/379 - 94s - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.1168 - val_accuracy: 0.9746 - 94s/epoch - 247ms/step\n",
            ">Saved modelsForStack/model_1.h5\n",
            "Model: \"Model1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 124)]        0           []                               \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " concat_input (Concatenate)     (None, 128)          0           ['input1[0][0]',                 \n",
            "                                                                  'input2[0][0]']                 \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, 128, 128)     1280512     ['concat_input[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 128, 128)     0           ['embedding_6[0][0]']            \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 128, 128)     0           ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " BiLSTM0 (Bidirectional)        (None, 128)          98816       ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " Dense_layer (Dense)            (None, 32)           4128        ['BiLSTM0[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 32)           0           ['Dense_layer[0][0]']            \n",
            "                                                                                                  \n",
            " Output (Dense)                 (None, 1)            33          ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,383,489\n",
            "Trainable params: 1,383,489\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "379/379 - 103s - loss: 0.2844 - accuracy: 0.8731 - val_loss: 0.1149 - val_accuracy: 0.9532 - 103s/epoch - 271ms/step\n",
            "Epoch 2/100\n",
            "379/379 - 93s - loss: 0.0655 - accuracy: 0.9779 - val_loss: 0.0386 - val_accuracy: 0.9848 - 93s/epoch - 247ms/step\n",
            "Epoch 3/100\n",
            "379/379 - 93s - loss: 0.0226 - accuracy: 0.9929 - val_loss: 0.0584 - val_accuracy: 0.9802 - 93s/epoch - 245ms/step\n",
            "Epoch 4/100\n",
            "379/379 - 93s - loss: 0.0118 - accuracy: 0.9964 - val_loss: 0.1198 - val_accuracy: 0.9647 - 93s/epoch - 245ms/step\n",
            "Epoch 5/100\n",
            "379/379 - 93s - loss: 0.0080 - accuracy: 0.9970 - val_loss: 0.1138 - val_accuracy: 0.9716 - 93s/epoch - 246ms/step\n",
            "Epoch 6/100\n",
            "379/379 - 93s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0598 - val_accuracy: 0.9858 - 93s/epoch - 245ms/step\n",
            "Epoch 7/100\n",
            "379/379 - 93s - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.0826 - val_accuracy: 0.9799 - 93s/epoch - 246ms/step\n",
            ">Saved modelsForStack/model_2.h5\n",
            "Model: \"Model1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 124)]        0           []                               \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " concat_input (Concatenate)     (None, 128)          0           ['input1[0][0]',                 \n",
            "                                                                  'input2[0][0]']                 \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, 128, 128)     1280512     ['concat_input[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 128, 128)     0           ['embedding_7[0][0]']            \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 128, 128)     0           ['dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " BiLSTM0 (Bidirectional)        (None, 128)          98816       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " Dense_layer (Dense)            (None, 32)           4128        ['BiLSTM0[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 32)           0           ['Dense_layer[0][0]']            \n",
            "                                                                                                  \n",
            " Output (Dense)                 (None, 1)            33          ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,383,489\n",
            "Trainable params: 1,383,489\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "379/379 - 105s - loss: 0.2827 - accuracy: 0.8797 - val_loss: 0.0831 - val_accuracy: 0.9687 - 105s/epoch - 276ms/step\n",
            "Epoch 2/100\n",
            "379/379 - 95s - loss: 0.0680 - accuracy: 0.9767 - val_loss: 0.0590 - val_accuracy: 0.9792 - 95s/epoch - 250ms/step\n",
            "Epoch 3/100\n",
            "379/379 - 95s - loss: 0.0270 - accuracy: 0.9912 - val_loss: 0.0416 - val_accuracy: 0.9835 - 95s/epoch - 250ms/step\n",
            "Epoch 4/100\n",
            "379/379 - 95s - loss: 0.0155 - accuracy: 0.9941 - val_loss: 0.1208 - val_accuracy: 0.9624 - 95s/epoch - 250ms/step\n",
            "Epoch 5/100\n",
            "379/379 - 95s - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.0728 - val_accuracy: 0.9795 - 95s/epoch - 250ms/step\n",
            "Epoch 6/100\n",
            "379/379 - 95s - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0849 - val_accuracy: 0.9792 - 95s/epoch - 250ms/step\n",
            "Epoch 7/100\n",
            "379/379 - 95s - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0739 - val_accuracy: 0.9832 - 95s/epoch - 250ms/step\n",
            "Epoch 8/100\n",
            "379/379 - 97s - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0922 - val_accuracy: 0.9786 - 97s/epoch - 255ms/step\n",
            ">Saved modelsForStack/model_3.h5\n",
            "Model: \"Model1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 124)]        0           []                               \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " concat_input (Concatenate)     (None, 128)          0           ['input1[0][0]',                 \n",
            "                                                                  'input2[0][0]']                 \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)        (None, 128, 128)     1280512     ['concat_input[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 128, 128)     0           ['embedding_8[0][0]']            \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 128, 128)     0           ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " BiLSTM0 (Bidirectional)        (None, 128)          98816       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " Dense_layer (Dense)            (None, 32)           4128        ['BiLSTM0[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 32)           0           ['Dense_layer[0][0]']            \n",
            "                                                                                                  \n",
            " Output (Dense)                 (None, 1)            33          ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,383,489\n",
            "Trainable params: 1,383,489\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "379/379 - 109s - loss: 0.2921 - accuracy: 0.8757 - val_loss: 0.0857 - val_accuracy: 0.9611 - 109s/epoch - 287ms/step\n",
            "Epoch 2/100\n",
            "379/379 - 96s - loss: 0.0689 - accuracy: 0.9774 - val_loss: 0.0514 - val_accuracy: 0.9795 - 96s/epoch - 253ms/step\n",
            "Epoch 3/100\n",
            "379/379 - 95s - loss: 0.0301 - accuracy: 0.9911 - val_loss: 0.0665 - val_accuracy: 0.9749 - 95s/epoch - 251ms/step\n",
            "Epoch 4/100\n",
            "379/379 - 94s - loss: 0.0142 - accuracy: 0.9958 - val_loss: 0.0830 - val_accuracy: 0.9739 - 94s/epoch - 248ms/step\n",
            "Epoch 5/100\n",
            "379/379 - 93s - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.0506 - val_accuracy: 0.9861 - 93s/epoch - 244ms/step\n",
            "Epoch 6/100\n",
            "379/379 - 93s - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0961 - val_accuracy: 0.9693 - 93s/epoch - 245ms/step\n",
            "Epoch 7/100\n",
            "379/379 - 93s - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.1525 - val_accuracy: 0.9650 - 93s/epoch - 245ms/step\n",
            "Epoch 8/100\n",
            "379/379 - 93s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0992 - val_accuracy: 0.9809 - 93s/epoch - 244ms/step\n",
            "Epoch 9/100\n",
            "379/379 - 93s - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0835 - val_accuracy: 0.9753 - 93s/epoch - 244ms/step\n",
            "Epoch 10/100\n",
            "379/379 - 93s - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.1712 - val_accuracy: 0.9647 - 93s/epoch - 245ms/step\n",
            ">Saved modelsForStack/model_4.h5\n",
            "Model: \"Model1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 124)]        0           []                               \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " concat_input (Concatenate)     (None, 128)          0           ['input1[0][0]',                 \n",
            "                                                                  'input2[0][0]']                 \n",
            "                                                                                                  \n",
            " embedding_9 (Embedding)        (None, 128, 128)     1280512     ['concat_input[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 128, 128)     0           ['embedding_9[0][0]']            \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 128, 128)     0           ['dropout_18[0][0]']             \n",
            "                                                                                                  \n",
            " BiLSTM0 (Bidirectional)        (None, 128)          98816       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " Dense_layer (Dense)            (None, 32)           4128        ['BiLSTM0[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 32)           0           ['Dense_layer[0][0]']            \n",
            "                                                                                                  \n",
            " Output (Dense)                 (None, 1)            33          ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,383,489\n",
            "Trainable params: 1,383,489\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "379/379 - 105s - loss: 0.2795 - accuracy: 0.8777 - val_loss: 0.1204 - val_accuracy: 0.9532 - 105s/epoch - 276ms/step\n",
            "Epoch 2/100\n",
            "379/379 - 93s - loss: 0.0622 - accuracy: 0.9812 - val_loss: 0.0786 - val_accuracy: 0.9733 - 93s/epoch - 246ms/step\n",
            "Epoch 3/100\n",
            "379/379 - 93s - loss: 0.0238 - accuracy: 0.9924 - val_loss: 0.0884 - val_accuracy: 0.9693 - 93s/epoch - 246ms/step\n",
            "Epoch 4/100\n",
            "379/379 - 93s - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.0251 - val_accuracy: 0.9918 - 93s/epoch - 246ms/step\n",
            "Epoch 5/100\n",
            "379/379 - 94s - loss: 0.0102 - accuracy: 0.9970 - val_loss: 0.1403 - val_accuracy: 0.9683 - 94s/epoch - 247ms/step\n",
            "Epoch 6/100\n",
            "379/379 - 94s - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.0610 - val_accuracy: 0.9812 - 94s/epoch - 247ms/step\n",
            "Epoch 7/100\n",
            "379/379 - 94s - loss: 0.0073 - accuracy: 0.9972 - val_loss: 0.1152 - val_accuracy: 0.9677 - 94s/epoch - 247ms/step\n",
            "Epoch 8/100\n",
            "379/379 - 94s - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0767 - val_accuracy: 0.9782 - 94s/epoch - 247ms/step\n",
            "Epoch 9/100\n",
            "379/379 - 94s - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.1006 - val_accuracy: 0.9762 - 94s/epoch - 247ms/step\n",
            ">Saved modelsForStack/model_5.h5\n"
          ]
        }
      ],
      "source": [
        "def fit_model(trainX, trainy):\n",
        "  # define model\n",
        "  callback = EarlyStopping(monitor='val_loss', patience=5)\n",
        "  input1 = Input(shape=(trn_x_encoded1.shape[1],), name=\"input1\")\n",
        "  input2 = Input(shape=(trn_x_sc1.shape[1],), name=\"input2\")\n",
        "  input = concatenate([input1, input2], axis=1, name=\"concat_input\")\n",
        "  x = Embedding(types_num + 4, 128, mask_zero=True, input_length=max_len)(input)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Activation('tanh')(x)\n",
        "  lstm = Bidirectional(LSTM(64, dropout=0.25), name=\"BiLSTM0\")(x)\n",
        "  dense = Dense(32, activation = \"relu\", name=\"Dense_layer\")(lstm)\n",
        "  drop = Dropout(0.25)(dense)\n",
        "  y = Dense(1, activation=\"sigmoid\", name=\"Output\")(drop)\n",
        "  model1 = Model(inputs= [input1, input2], outputs=y, name=\"Model1\")\n",
        "  model1.summary()\n",
        "  model1.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "  # fit the model\n",
        "  model1.fit([trn_x_encoded1, trn_x_sc1], trn_y_encoded1, batch_size=32, epochs=100,\n",
        "           validation_split=0.2, callbacks=[callback], verbose=2)\n",
        "\t\n",
        "  return model\n",
        "\n",
        "\n",
        "# save submodels\n",
        "for i in range(5):\n",
        "  # fit the model\n",
        "  model = fit_model(trn_x_encoded_with_senti, trn_y_encoded1)\n",
        "  filename = 'modelsForStack/model_' + str(i + 1) + '.h5'\n",
        "  model.save(filename)\n",
        "  print('>Saved %s' % filename)\n",
        "\n",
        "\n",
        "# load self-chosen numbers of submodels\n",
        "def load_submodels(n_models):\n",
        "  submodels = []\n",
        "  for i in range(n_models):\n",
        "    filename = 'modelsForStack/model_' + str(i + 1) + '.h5'\n",
        "    model = load_model(filename)\n",
        "    submodels.append(model)\n",
        "  return submodels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlJ7sKx2uMSX"
      },
      "outputs": [],
      "source": [
        "# or we can use the predicted results for non-nn models, 3d flatten into 2d result\n",
        "def stacked_preds(trn_x, trn_y, tst_x, num_model, tst_y=None):\n",
        "  stacked_pred = None\n",
        "  for i in range(num_model):\n",
        "    model = fit_model(trn_x, trn_y)\n",
        "    pred = model.predict(tst_x)\n",
        "    if stacked_pred is None:\n",
        "      stacked_pred =  pred\n",
        "    else:\n",
        "      stacked_pred = np.dstack((stacked_pred, pred))\n",
        "  print(\"The shape of stacked_pred:{}\".format(stacked_pred.shape))\n",
        "  stacked_pred = stacked_pred.reshape((stacked_pred.shape[0], stacked_pred.shape[1]*stacked_pred.shape[2]))\n",
        "  return stacked_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4em3LpM3IwWx"
      },
      "outputs": [],
      "source": [
        "nn5_preds = stacked_preds(trn_x_encoded_with_senti, trn_y_encoded1, tst_x_encoded_with_senti, 5)\n",
        "nn5_preds = pd.DataFrame(nn5_preds)\n",
        "nn5_preds.to_csv(\"trn_5nn_predictions.csv\", encoding=\"utf-8\") # change to trn_x in predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcD5M4s-DneA",
        "outputId": "abcd4b7b-702f-4674-963e-fc47de6724dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<keras.engine.sequential.Sequential object at 0x7f2030612050>, <keras.engine.sequential.Sequential object at 0x7f2030d17f50>, <keras.engine.sequential.Sequential object at 0x7f2030cfa110>, <keras.engine.sequential.Sequential object at 0x7f203053e550>, <keras.engine.sequential.Sequential object at 0x7f203061d910>]\n"
          ]
        }
      ],
      "source": [
        "# use the output of the submodels as an input for the meta model\n",
        "submodels = load_submodels(5)\n",
        "print(submodels)\n",
        "\n",
        "def ensemble_model(submodels):\n",
        "  # first make the submodels untrainable during the meta model training\n",
        "  for i in range(len(submodels)):\n",
        "\t  model = submodels[i]\n",
        "\t  for layer in model.layers:\n",
        "\t\t  # make not trainable\n",
        "\t\t  layer.trainable = False\n",
        "\t\t  # rename to avoid 'unique layer name' issue\n",
        "\t\t  layer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
        "  inputs_submodel = [model.input for model in submodels]\n",
        "  #input_emoji = Input(shape=(trn_x1_emoji.shape[1],), name='input_emoji')\n",
        "  print(\"submodel inputs: {}\".format(inputs_submodel))\n",
        "  outputs_submodel = [model.output for model in submodels]\n",
        "  concat = concatenate(outputs_submodel, name='concat_submodel_outputs')\n",
        "  # we will use the emoji predictions as extra embedding\n",
        "  #concat = concatenate([concat, input_emoji], name = 'concat_emoji')\n",
        "  print(\"concat output: {}\".format(concat))\n",
        "  x = Embedding(types_num, 64, mask_zero=True, \n",
        "                input_length=concat.shape[1])(concat)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Bidirectional(LSTM(128, dropout=0.25), name='BiLSTM')(x)\n",
        "  x = Dense(32, activation='relu', name='Dense_layer')(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  y = Dense(1, activation=\"sigmoid\", name=\"Output\")(x)\n",
        "  model = Model(inputs=inputs_submodel, outputs = y, name= 'Model_ensemble1')\n",
        "  # plot graph of ensemble\n",
        "  #plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
        "  model.summary()\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                 optimizer=Adam(learning_rate=0.001),\n",
        "                 metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhkMyzz7WKyM"
      },
      "outputs": [],
      "source": [
        "# fit the model and get predicitions\n",
        "def fit_ensemble_model(ensemble_model, x, y):\n",
        "  callback = EarlyStopping(monitor='val_loss', patience=5)\n",
        "  inputs = [x for _ in range(len(ensemble_model.input))]\n",
        "  #inputs =  inputs + trn_x1_emoji\n",
        "  ensemble_model.fit(inputs, y, batch_size=50, epochs=100,\n",
        "           validation_split=0.2, callbacks=[callback], verbose=2)\n",
        "\n",
        "def predict_ensemble_model(ensemble_model, x):\n",
        "  inputs = [x for _ in range(len(ensemble_model.input))]\n",
        "  #inputs = inputs + [tst_x1_emoji]\n",
        "  return ensemble_model.predict(inputs, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPBGTP7QbkAh",
        "outputId": "1513761c-8104-4069-9db7-cedee9d329b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submodel inputs: [<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'dense_input')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'dense_2_input')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'dense_4_input')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'dense_6_input')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'dense_8_input')>]\n",
            "concat output: KerasTensor(type_spec=TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), name='concat_submodel_outputs/concat:0', description=\"created by layer 'concat_submodel_outputs'\")\n",
            "Model: \"Model_ensemble1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " dense_input (InputLayer)       [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " dense_2_input (InputLayer)     [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " dense_4_input (InputLayer)     [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " dense_6_input (InputLayer)     [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " dense_8_input (InputLayer)     [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " ensemble_1_dense (Dense)       (None, 64)           8256        ['dense_input[0][0]']            \n",
            "                                                                                                  \n",
            " ensemble_2_dense_2 (Dense)     (None, 64)           8256        ['dense_2_input[0][0]']          \n",
            "                                                                                                  \n",
            " ensemble_3_dense_4 (Dense)     (None, 64)           8256        ['dense_4_input[0][0]']          \n",
            "                                                                                                  \n",
            " ensemble_4_dense_6 (Dense)     (None, 64)           8256        ['dense_6_input[0][0]']          \n",
            "                                                                                                  \n",
            " ensemble_5_dense_8 (Dense)     (None, 64)           8256        ['dense_8_input[0][0]']          \n",
            "                                                                                                  \n",
            " ensemble_1_dense_1 (Dense)     (None, 1)            65          ['ensemble_1_dense[0][0]']       \n",
            "                                                                                                  \n",
            " ensemble_2_dense_3 (Dense)     (None, 1)            65          ['ensemble_2_dense_2[0][0]']     \n",
            "                                                                                                  \n",
            " ensemble_3_dense_5 (Dense)     (None, 1)            65          ['ensemble_3_dense_4[0][0]']     \n",
            "                                                                                                  \n",
            " ensemble_4_dense_7 (Dense)     (None, 1)            65          ['ensemble_4_dense_6[0][0]']     \n",
            "                                                                                                  \n",
            " ensemble_5_dense_9 (Dense)     (None, 1)            65          ['ensemble_5_dense_8[0][0]']     \n",
            "                                                                                                  \n",
            " concat_submodel_outputs (Conca  (None, 5)           0           ['ensemble_1_dense_1[0][0]',     \n",
            " tenate)                                                          'ensemble_2_dense_3[0][0]',     \n",
            "                                                                  'ensemble_3_dense_5[0][0]',     \n",
            "                                                                  'ensemble_4_dense_7[0][0]',     \n",
            "                                                                  'ensemble_5_dense_9[0][0]']     \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 5, 64)        640000      ['concat_submodel_outputs[0][0]']\n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 5, 64)        0           ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " BiLSTM (Bidirectional)         (None, 256)          197632      ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " Dense_layer (Dense)            (None, 32)           8224        ['BiLSTM[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 32)           0           ['Dense_layer[0][0]']            \n",
            "                                                                                                  \n",
            " Output (Dense)                 (None, 1)            33          ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 887,494\n",
            "Trainable params: 845,889\n",
            "Non-trainable params: 41,605\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "243/243 - 20s - loss: 0.6081 - accuracy: 0.6871 - val_loss: 0.9134 - val_accuracy: 0.2088 - 20s/epoch - 81ms/step\n",
            "Epoch 2/100\n",
            "243/243 - 8s - loss: 0.5951 - accuracy: 0.6961 - val_loss: 0.8573 - val_accuracy: 0.2088 - 8s/epoch - 31ms/step\n",
            "Epoch 3/100\n",
            "243/243 - 8s - loss: 0.5933 - accuracy: 0.6961 - val_loss: 0.9169 - val_accuracy: 0.2088 - 8s/epoch - 31ms/step\n",
            "Epoch 4/100\n",
            "243/243 - 8s - loss: 0.5950 - accuracy: 0.6961 - val_loss: 0.8664 - val_accuracy: 0.2088 - 8s/epoch - 31ms/step\n",
            "Epoch 5/100\n",
            "243/243 - 8s - loss: 0.5938 - accuracy: 0.6961 - val_loss: 0.8917 - val_accuracy: 0.2088 - 8s/epoch - 31ms/step\n",
            "Epoch 6/100\n",
            "243/243 - 7s - loss: 0.5936 - accuracy: 0.6961 - val_loss: 0.9044 - val_accuracy: 0.2088 - 7s/epoch - 31ms/step\n",
            "Epoch 7/100\n",
            "243/243 - 8s - loss: 0.5930 - accuracy: 0.6961 - val_loss: 0.8520 - val_accuracy: 0.2088 - 8s/epoch - 31ms/step\n",
            "Epoch 8/100\n",
            "243/243 - 7s - loss: 0.5947 - accuracy: 0.6961 - val_loss: 0.8596 - val_accuracy: 0.2088 - 7s/epoch - 31ms/step\n",
            "Epoch 9/100\n",
            "243/243 - 8s - loss: 0.5935 - accuracy: 0.6961 - val_loss: 0.8845 - val_accuracy: 0.2088 - 8s/epoch - 35ms/step\n",
            "Epoch 10/100\n",
            "243/243 - 7s - loss: 0.5932 - accuracy: 0.6961 - val_loss: 0.8861 - val_accuracy: 0.2088 - 7s/epoch - 30ms/step\n",
            "Epoch 11/100\n",
            "243/243 - 7s - loss: 0.5922 - accuracy: 0.6961 - val_loss: 0.9062 - val_accuracy: 0.2088 - 7s/epoch - 31ms/step\n",
            "Epoch 12/100\n",
            "243/243 - 8s - loss: 0.5930 - accuracy: 0.6961 - val_loss: 0.9614 - val_accuracy: 0.2088 - 8s/epoch - 31ms/step\n"
          ]
        }
      ],
      "source": [
        "ensemble_model = ensemble_model(submodels)\n",
        "fit_ensemble_model(ensemble_model, trn_x_encoded_with_senti, trn_y_encoded1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xUBMomiRi5M"
      },
      "outputs": [],
      "source": [
        "ensemble_pred = predict_ensemble_model(ensemble_model, tst_x_encoded_with_senti)\n",
        "print(\"The predicitons of the ensemble model: {}\".format(ensemble_pred))\n",
        "ensemble_pred = [1 if p[0] >= 0.5 else 0 for p in ensemble_pred]\n",
        "cls_report = classification_report(tst_y_encoded1, ensemble_pred)\n",
        "f1_ensem = f1_score(tst_y_encoded1, ensemble_pred)\n",
        "print(cls_report), f1_ensem #0.32 ensemble without senti-score, 0.38 with senti-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJyBLpoYMWDt"
      },
      "source": [
        "## Functional API Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuaPdXemMVLM",
        "outputId": "1b098d77-a3d8-4ec1-8f5b-704b45cdd65d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Model1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 166)]        0           []                               \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " concat_input (Concatenate)     (None, 170)          0           ['input1[0][0]',                 \n",
            "                                                                  'input2[0][0]']                 \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 170, 256)     2561024     ['concat_input[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 170, 256)     0           ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 170, 256)     0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " BiLSTM0 (Bidirectional)        (None, 170, 1024)    3149824     ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " BiLSTM1 (Bidirectional)        (None, 170, 1024)    6295552     ['BiLSTM0[0][0]']                \n",
            "                                                                                                  \n",
            " concat_lstm_embeddi (Concatena  (None, 170, 2304)   0           ['BiLSTM0[0][0]',                \n",
            " te)                                                              'BiLSTM1[0][0]',                \n",
            "                                                                  'activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " attention_layer (AttentionWeig  (None, 2304)        2304        ['concat_lstm_embeddi[0][0]']    \n",
            " htedAverage)                                                                                     \n",
            "                                                                                                  \n",
            " Dense_layer (Dense)            (None, 128)          295040      ['attention_layer[0][0]']        \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 128)          0           ['Dense_layer[0][0]']            \n",
            "                                                                                                  \n",
            " Output (Dense)                 (None, 1)            129         ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 12,303,873\n",
            "Trainable params: 12,303,873\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "49/49 - 2978s - loss: 0.3678 - accuracy: 0.8306 - val_loss: 0.1391 - val_accuracy: 0.9489 - 2978s/epoch - 61s/step\n",
            "Epoch 2/30\n",
            "49/49 - 2931s - loss: 0.0690 - accuracy: 0.9773 - val_loss: 0.1151 - val_accuracy: 0.9640 - 2931s/epoch - 60s/step\n",
            "Epoch 3/30\n",
            "49/49 - 2936s - loss: 0.0277 - accuracy: 0.9919 - val_loss: 0.1834 - val_accuracy: 0.9492 - 2936s/epoch - 60s/step\n",
            "Epoch 4/30\n",
            "49/49 - 2933s - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.2018 - val_accuracy: 0.9528 - 2933s/epoch - 60s/step\n",
            "Epoch 5/30\n",
            "49/49 - 2949s - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.2535 - val_accuracy: 0.9462 - 2949s/epoch - 60s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7febf2b65b50>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "input1 = Input(shape=(trn_x_encoded1.shape[1],), name=\"input1\")\n",
        "input2 = Input(shape=(trn_x_sc1.shape[1],), name=\"input2\")\n",
        "input = concatenate([input1, input2], axis=1, name=\"concat_input\")\n",
        "x = Embedding(types_num + 4, 256, mask_zero=True, input_length=max_len)(input)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Activation('tanh')(x)\n",
        "lstm0 = Bidirectional(LSTM(512, return_sequences=True,dropout=0.25), name=\"BiLSTM0\")(x)\n",
        "lstm1 = Bidirectional(LSTM(512, return_sequences=True, dropout=0.25), name=\"BiLSTM1\")(lstm0)\n",
        "concat = concatenate([lstm0, lstm1, x], name=\"concat_lstm_embeddi\")\n",
        "attn = AttentionWeightedAverage(name=\"attention_layer\")(concat)\n",
        "dense = Dense(128, activation = \"relu\", name=\"Dense_layer\")(attn)\n",
        "drop = Dropout(0.25)(dense)\n",
        "y = Dense(1, activation=\"sigmoid\", name=\"Output\")(drop)\n",
        "model1 = Model(inputs= [input1, input2], outputs=y, name=\"Model1\")\n",
        "model1.summary()\n",
        "model1.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "model1.fit([trn_x_encoded1, trn_x_sc1], trn_y_encoded1, batch_size=250, epochs=30,\n",
        "           validation_split=0.2, callbacks=[callback], verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBBH0SPLIzMv"
      },
      "outputs": [],
      "source": [
        "pred1 = model1.predict([tst_x_encoded1, tst_x_sc1])\n",
        "pred1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuSwOZJGKKHg",
        "outputId": "2b72aeb5-3cc0-4d14-bcc9-432f3733ec88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 0, 0, 0])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_labels = np.array([1 if p[0] >= 0.5 else 0 for p in pred1])\n",
        "pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lhuO420tije"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22va3h7ZI82b",
        "outputId": "dbe09738-dd3e-4c8f-bdeb-f2d894f7e89f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('              precision    recall  f1-score   support\\n\\n           0       0.93      0.91      0.92      1895\\n           1       0.29      0.35      0.32       199\\n\\n    accuracy                           0.86      2094\\n   macro avg       0.61      0.63      0.62      2094\\nweighted avg       0.87      0.86      0.86      2094\\n',\n",
              " 0.6180386984523426)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_sc = f1_score(tst_y_encoded1, pred_labels, average='binary')\n",
        "cr = classification_report(tst_y_encoded1, pred_labels)\n",
        "f1_sc, print(cr) #0.578603 macro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giI6d59Kg0ly"
      },
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtzWajvbiClL"
      },
      "source": [
        "## Load data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQPfm1YYDhp6"
      },
      "outputs": [],
      "source": [
        "from ast import literal_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ncyuVLHKgz8Z",
        "outputId": "742c0f3b-2af5-46ac-fba8-1442a477f4ac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-633bb099-bcb4-460a-aaec-73d29e97e46a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>par_id</th>\n",
              "      <th>text</th>\n",
              "      <th>country</th>\n",
              "      <th>keyword</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4046</td>\n",
              "      <td>we also know that they can benefit by receivin...</td>\n",
              "      <td>us</td>\n",
              "      <td>hopeless</td>\n",
              "      <td>[1, 0, 0, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1279</td>\n",
              "      <td>pope francis washed and kissed the feet of mus...</td>\n",
              "      <td>ng</td>\n",
              "      <td>refugee</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8330</td>\n",
              "      <td>many refugees do n't want to be resettled anyw...</td>\n",
              "      <td>ng</td>\n",
              "      <td>refugee</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4063</td>\n",
              "      <td>\"budding chefs , like \"\" fred \"\" , \"\" winston ...</td>\n",
              "      <td>ie</td>\n",
              "      <td>in-need</td>\n",
              "      <td>[1, 0, 0, 1, 1, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4089</td>\n",
              "      <td>\"in a 90-degree view of his constituency , one...</td>\n",
              "      <td>pk</td>\n",
              "      <td>homeless</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>8638</td>\n",
              "      <td>gregory chen , director of government relation...</td>\n",
              "      <td>nz</td>\n",
              "      <td>immigrant</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>8639</td>\n",
              "      <td>foreign tourists rest on the floor while waiti...</td>\n",
              "      <td>ph</td>\n",
              "      <td>homeless</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>8640</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ke</td>\n",
              "      <td>migrant</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>8641</td>\n",
              "      <td>approve name-change or face ' hopelessness ' :...</td>\n",
              "      <td>ph</td>\n",
              "      <td>hopeless</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>8642</td>\n",
              "      <td>the united methodist church where undocumented...</td>\n",
              "      <td>ph</td>\n",
              "      <td>immigrant</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>437 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-633bb099-bcb4-460a-aaec-73d29e97e46a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-633bb099-bcb4-460a-aaec-73d29e97e46a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-633bb099-bcb4-460a-aaec-73d29e97e46a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     par_id  ...                  label\n",
              "0      4046  ...  [1, 0, 0, 1, 0, 0, 0]\n",
              "1      1279  ...  [0, 1, 0, 0, 0, 0, 0]\n",
              "2      8330  ...  [0, 0, 1, 0, 0, 0, 0]\n",
              "3      4063  ...  [1, 0, 0, 1, 1, 1, 0]\n",
              "4      4089  ...  [1, 0, 0, 0, 0, 0, 0]\n",
              "..      ...  ...                    ...\n",
              "432    8638  ...  [0, 0, 0, 0, 0, 0, 0]\n",
              "433    8639  ...  [0, 0, 0, 0, 0, 0, 0]\n",
              "434    8640  ...  [0, 0, 0, 0, 0, 0, 0]\n",
              "435    8641  ...  [0, 0, 0, 0, 0, 0, 0]\n",
              "436    8642  ...  [0, 0, 0, 0, 0, 0, 0]\n",
              "\n",
              "[437 rows x 5 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trndf2 = pd.read_csv(\"trndf2.csv\")\n",
        "tstdf2 = pd.read_csv(\"tstdf2.csv\")\n",
        "trndf2.label = trndf2.label.apply(literal_eval)\n",
        "tstdf2.label = tstdf2.label.apply(literal_eval)\n",
        "tstdf2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "vhck5tJSiTd-",
        "outputId": "999be0e4-5f4d-45dd-e724-c88657841463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "434 nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn_x2 = trndf2[\"text\"]\n",
        "tst_x2 = tstdf2[\"text\"]\n",
        "trn_y2 = trndf2[\"label\"]\n",
        "tst_y2 = tstdf2[\"label\"]\n",
        "for i, t in enumerate(tst_x2):\n",
        "  if type(t) is float:\n",
        "    print(i, t)\n",
        "tst_x2.loc[434] = ''\n",
        "tst_x2.loc[434]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x8nPCdjjeay",
        "outputId": "7eea35d0-db44-4570-807d-ac286c3e0150"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1032, 132), (437, 132), 132)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn_x_encoded2, tst_x_encoded2, max_len2 = tok_vec(types_num, trn_x2, tst_x2)\n",
        "trn_x_encoded2.shape, tst_x_encoded2.shape, max_len2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iowdsMZiCA7a",
        "outputId": "623103e2-2f43-4f98-a78c-84bf5ff2b438"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[1, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 1, 0, ..., 0, 0, 0],\n",
              "        [1, 0, 0, ..., 0, 1, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]), (437, 7))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn_y_encoded2 = np.array([list(l) for l in trn_y2.values])\n",
        "tst_y_encoded2 = np.array([list(l) for l in tst_y2.values])\n",
        "trn_y_encoded2, tst_y_encoded2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-BSeWoG-RfO"
      },
      "outputs": [],
      "source": [
        "#class_weights2 = class_weight.compute_class_weight(class_weight='balanced', \n",
        "                                                   #classes=np.unique(trn_y_encoded2), y=trn_y_encoded2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M5wfh0Qb-Fvt",
        "outputId": "8139a69a-6b38-4957-a5ea-089fd025afdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbacktranslation_subtask2\u001b[0m/         rebuilddata.py\n",
            "data1                             sentiment_scoring.py\n",
            "data2.pickle                      ten-langs-regex-cleaned-binary-label.csv\n",
            "dev_semeval_parids-labels.csv     \u001b[01;34mthePoorerTheMerrier\u001b[0m/\n",
            "dontpatronizeme_categories.tsv    tptm.csv\n",
            "dontpatronizeme_pcl.tsv           train_semeval_parids-labels.csv\n",
            "dont_patronize_me.py              trndf1.csv\n",
            "emoji_feature_vectors_trn_x1.csv  trndf2.csv\n",
            "emoji_feature_vectors_tst_x1.csv  trn_encoded_with_senti_df1\n",
            "pcl.ipynb                         trn_x1_augmented\n",
            "pcl-task1-augment-revised.ipynb   trn_x1_original\n",
            "pcl-task2-augment.ipynb           tstdf1.csv\n",
            "\u001b[01;34m__pycache__\u001b[0m/                      tstdf2.csv\n",
            "README.txt                        tst_x1_original\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting flair\n",
            "  Downloading flair-0.10-py3-none-any.whl (322 kB)\n",
            "\u001b[K     |████████████████████████████████| 322 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.2)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 6.2 MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.62.3)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Collecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.10.0+cu111)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 63.3 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.0.0\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 44.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.13.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.10.0.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (3.0.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 75.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 65.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, sqlitedict, ftfy, langdetect, wikipedia-api\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9704 sha256=c2fa438e753dd3d25489bb6f0c6b6588facef5a35a1d8d97c8cee5d704f2fcbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=79d4cd86585ee1dd6685379fa26b86958aa55e4e9275dd433a95f82158711811\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=c751bf844b4ed4f56f6f6adc9926fb5275ca4d0c3fa476274e2cd551583a1f34\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14393 sha256=f991df35ddb8b419e21c99edf977e8c505432bafca4fc4e8ea3f23f313bd8e9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=d35ed1424652b0e9a2dd5c936d40c475d403f954f2736f5eab834ed1dd59d793\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=5c376ab0ada82a36298f38c1f30b25ab4a40fb9b490fa907d6c30ecc3ce4a668\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=f871de5b6baacdbaed4c7730fa6d4d4b45e33fc756087b72bc2380f8cf54c2a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built gdown mpld3 overrides sqlitedict ftfy langdetect wikipedia-api\n",
            "Installing collected packages: requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, mpld3, more-itertools, langdetect, konoha, janome, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.10.0\n",
            "    Uninstalling importlib-metadata-4.10.0:\n",
            "      Successfully uninstalled importlib-metadata-4.10.0\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.10 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.4.0 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pyyaml-6.0 requests-2.27.1 sacremoses-0.0.47 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 transformers-4.15.0 wikipedia-api-0.5.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "((1032, 4), array([[ 0.    ,  0.804 ,  0.196 ,  0.9515],\n",
              "        [ 0.    ,  0.934 ,  0.066 ,  0.3818],\n",
              "        [ 0.187 ,  0.813 ,  0.    , -0.3009],\n",
              "        ...,\n",
              "        [ 0.    ,  0.    ,  0.    ,  0.    ],\n",
              "        [ 0.313 ,  0.687 ,  0.    , -0.6249],\n",
              "        [ 0.    ,  0.877 ,  0.123 ,  0.4215]]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%ls\n",
        "!pip install vaderSentiment\n",
        "!pip install flair\n",
        "from sentiment_scoring import senti_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "trn_x_sc2 = np.array(senti_score(trn_x2, \"vader\"))\n",
        "tst_x_sc2 = np.array(senti_score(tst_x2, \"vader\"))\n",
        "trn_x_sc2.shape, tst_x_sc2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkSjSwP9-VSB",
        "outputId": "2f4a8896-bd12-425d-fe88-413a38f106f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Model2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 132)]        0           []                               \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " concat_input (Concatenate)     (None, 136)          0           ['input1[0][0]',                 \n",
            "                                                                  'input2[0][0]']                 \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 136, 256)     1281024     ['concat_input[0][0]']           \n",
            "                                                                                                  \n",
            " BiLSTM (Bidirectional)         (None, 1024)         3149824     ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " Dense_layer (Dense)            (None, 128)          131200      ['BiLSTM[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128)          0           ['Dense_layer[0][0]']            \n",
            "                                                                                                  \n",
            " Output (Dense)                 (None, 7)            903         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,562,951\n",
            "Trainable params: 4,562,951\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "26/26 - 126s - loss: 0.5549 - accuracy: 0.6024 - val_loss: 0.4310 - val_accuracy: 1.0000 - 126s/epoch - 5s/step\n",
            "Epoch 2/100\n",
            "26/26 - 114s - loss: 0.4943 - accuracy: 0.7285 - val_loss: 0.4848 - val_accuracy: 0.8454 - 114s/epoch - 4s/step\n",
            "Epoch 3/100\n",
            "26/26 - 114s - loss: 0.4507 - accuracy: 0.6897 - val_loss: 0.4013 - val_accuracy: 0.6329 - 114s/epoch - 4s/step\n",
            "Epoch 4/100\n",
            "26/26 - 116s - loss: 0.3749 - accuracy: 0.6655 - val_loss: 0.3659 - val_accuracy: 0.6908 - 116s/epoch - 4s/step\n",
            "Epoch 5/100\n",
            "26/26 - 121s - loss: 0.3137 - accuracy: 0.6679 - val_loss: 0.4777 - val_accuracy: 0.6329 - 121s/epoch - 5s/step\n",
            "Epoch 6/100\n",
            "26/26 - 117s - loss: 0.2425 - accuracy: 0.6776 - val_loss: 0.5730 - val_accuracy: 0.6570 - 117s/epoch - 4s/step\n",
            "Epoch 7/100\n",
            "26/26 - 116s - loss: 0.1788 - accuracy: 0.6667 - val_loss: 0.6829 - val_accuracy: 0.7681 - 116s/epoch - 4s/step\n",
            "Epoch 8/100\n",
            "26/26 - 113s - loss: 0.1383 - accuracy: 0.6921 - val_loss: 0.8594 - val_accuracy: 0.6570 - 113s/epoch - 4s/step\n",
            "Epoch 9/100\n",
            "26/26 - 112s - loss: 0.1070 - accuracy: 0.6752 - val_loss: 0.7263 - val_accuracy: 0.6280 - 112s/epoch - 4s/step\n",
            "Epoch 10/100\n",
            "26/26 - 112s - loss: 0.0806 - accuracy: 0.6788 - val_loss: 1.0376 - val_accuracy: 0.7101 - 112s/epoch - 4s/step\n",
            "Epoch 11/100\n",
            "26/26 - 113s - loss: 0.0544 - accuracy: 0.6824 - val_loss: 1.2128 - val_accuracy: 0.6087 - 113s/epoch - 4s/step\n",
            "Epoch 12/100\n",
            "26/26 - 112s - loss: 0.0446 - accuracy: 0.6824 - val_loss: 1.0746 - val_accuracy: 0.6860 - 112s/epoch - 4s/step\n",
            "Epoch 13/100\n",
            "26/26 - 114s - loss: 0.0401 - accuracy: 0.6582 - val_loss: 1.1892 - val_accuracy: 0.6280 - 114s/epoch - 4s/step\n",
            "Epoch 14/100\n",
            "26/26 - 114s - loss: 0.0333 - accuracy: 0.6982 - val_loss: 1.1791 - val_accuracy: 0.5845 - 114s/epoch - 4s/step\n",
            "Epoch 15/100\n",
            "26/26 - 113s - loss: 0.0279 - accuracy: 0.6594 - val_loss: 1.2821 - val_accuracy: 0.5990 - 113s/epoch - 4s/step\n",
            "Epoch 16/100\n",
            "26/26 - 113s - loss: 0.0202 - accuracy: 0.6012 - val_loss: 1.1823 - val_accuracy: 0.6812 - 113s/epoch - 4s/step\n",
            "Epoch 17/100\n",
            "26/26 - 113s - loss: 0.0138 - accuracy: 0.6691 - val_loss: 1.3757 - val_accuracy: 0.6473 - 113s/epoch - 4s/step\n",
            "Epoch 18/100\n",
            "26/26 - 114s - loss: 0.0109 - accuracy: 0.6570 - val_loss: 1.5738 - val_accuracy: 0.6570 - 114s/epoch - 4s/step\n",
            "Epoch 19/100\n",
            "26/26 - 114s - loss: 0.0098 - accuracy: 0.6848 - val_loss: 1.5121 - val_accuracy: 0.5749 - 114s/epoch - 4s/step\n",
            "Epoch 20/100\n",
            "26/26 - 113s - loss: 0.0165 - accuracy: 0.6242 - val_loss: 1.2388 - val_accuracy: 0.6425 - 113s/epoch - 4s/step\n",
            "Epoch 21/100\n",
            "26/26 - 114s - loss: 0.0104 - accuracy: 0.6109 - val_loss: 1.4865 - val_accuracy: 0.7053 - 114s/epoch - 4s/step\n",
            "Epoch 22/100\n",
            "26/26 - 112s - loss: 0.0113 - accuracy: 0.6812 - val_loss: 1.4376 - val_accuracy: 0.6280 - 112s/epoch - 4s/step\n",
            "Epoch 23/100\n",
            "26/26 - 113s - loss: 0.0087 - accuracy: 0.6752 - val_loss: 1.7679 - val_accuracy: 0.7246 - 113s/epoch - 4s/step\n",
            "Epoch 24/100\n",
            "26/26 - 113s - loss: 0.0088 - accuracy: 0.6533 - val_loss: 1.7598 - val_accuracy: 0.5556 - 113s/epoch - 4s/step\n",
            "Epoch 25/100\n",
            "26/26 - 112s - loss: 0.0062 - accuracy: 0.6036 - val_loss: 1.5805 - val_accuracy: 0.5507 - 112s/epoch - 4s/step\n",
            "Epoch 26/100\n",
            "26/26 - 113s - loss: 0.0070 - accuracy: 0.6279 - val_loss: 1.6914 - val_accuracy: 0.6377 - 113s/epoch - 4s/step\n",
            "Epoch 27/100\n",
            "26/26 - 113s - loss: 0.0168 - accuracy: 0.6315 - val_loss: 1.5371 - val_accuracy: 0.6425 - 113s/epoch - 4s/step\n",
            "Epoch 28/100\n",
            "26/26 - 112s - loss: 0.0178 - accuracy: 0.6315 - val_loss: 1.3291 - val_accuracy: 0.5266 - 112s/epoch - 4s/step\n",
            "Epoch 29/100\n",
            "26/26 - 113s - loss: 0.0133 - accuracy: 0.6315 - val_loss: 1.4978 - val_accuracy: 0.6135 - 113s/epoch - 4s/step\n",
            "Epoch 30/100\n",
            "26/26 - 118s - loss: 0.0060 - accuracy: 0.6194 - val_loss: 1.4828 - val_accuracy: 0.6184 - 118s/epoch - 5s/step\n",
            "Epoch 31/100\n",
            "26/26 - 120s - loss: 0.0053 - accuracy: 0.6364 - val_loss: 1.7428 - val_accuracy: 0.6473 - 120s/epoch - 5s/step\n",
            "Epoch 32/100\n",
            "26/26 - 116s - loss: 0.0064 - accuracy: 0.6836 - val_loss: 1.3463 - val_accuracy: 0.5314 - 116s/epoch - 4s/step\n",
            "Epoch 33/100\n",
            "26/26 - 114s - loss: 0.0081 - accuracy: 0.5964 - val_loss: 1.5921 - val_accuracy: 0.5797 - 114s/epoch - 4s/step\n",
            "Epoch 34/100\n",
            "26/26 - 120s - loss: 0.0095 - accuracy: 0.6739 - val_loss: 1.4445 - val_accuracy: 0.6039 - 120s/epoch - 5s/step\n",
            "Epoch 35/100\n",
            "26/26 - 115s - loss: 0.0069 - accuracy: 0.5988 - val_loss: 1.4991 - val_accuracy: 0.5217 - 115s/epoch - 4s/step\n",
            "Epoch 36/100\n",
            "26/26 - 115s - loss: 0.0053 - accuracy: 0.5612 - val_loss: 1.5043 - val_accuracy: 0.5894 - 115s/epoch - 4s/step\n",
            "Epoch 37/100\n",
            "26/26 - 116s - loss: 0.0088 - accuracy: 0.5794 - val_loss: 1.4091 - val_accuracy: 0.5507 - 116s/epoch - 4s/step\n",
            "Epoch 38/100\n",
            "26/26 - 114s - loss: 0.0080 - accuracy: 0.5964 - val_loss: 1.5893 - val_accuracy: 0.5507 - 114s/epoch - 4s/step\n",
            "Epoch 39/100\n",
            "26/26 - 113s - loss: 0.0090 - accuracy: 0.6267 - val_loss: 1.4219 - val_accuracy: 0.5459 - 113s/epoch - 4s/step\n",
            "Epoch 40/100\n",
            "26/26 - 113s - loss: 0.0087 - accuracy: 0.6788 - val_loss: 1.6377 - val_accuracy: 0.6618 - 113s/epoch - 4s/step\n",
            "Epoch 41/100\n",
            "26/26 - 113s - loss: 0.0050 - accuracy: 0.5952 - val_loss: 1.5188 - val_accuracy: 0.5121 - 113s/epoch - 4s/step\n",
            "Epoch 42/100\n",
            "26/26 - 116s - loss: 0.0029 - accuracy: 0.5685 - val_loss: 1.7837 - val_accuracy: 0.5507 - 116s/epoch - 4s/step\n",
            "Epoch 43/100\n",
            "26/26 - 115s - loss: 0.0026 - accuracy: 0.6000 - val_loss: 1.8541 - val_accuracy: 0.6280 - 115s/epoch - 4s/step\n",
            "Epoch 44/100\n",
            "26/26 - 114s - loss: 0.0035 - accuracy: 0.5988 - val_loss: 1.8038 - val_accuracy: 0.5942 - 114s/epoch - 4s/step\n",
            "Epoch 45/100\n",
            "26/26 - 113s - loss: 0.0037 - accuracy: 0.5794 - val_loss: 1.8061 - val_accuracy: 0.5845 - 113s/epoch - 4s/step\n",
            "Epoch 46/100\n",
            "26/26 - 114s - loss: 0.0051 - accuracy: 0.5745 - val_loss: 2.0363 - val_accuracy: 0.6184 - 114s/epoch - 4s/step\n",
            "Epoch 47/100\n",
            "26/26 - 114s - loss: 0.0060 - accuracy: 0.6424 - val_loss: 1.6930 - val_accuracy: 0.5700 - 114s/epoch - 4s/step\n",
            "Epoch 48/100\n",
            "26/26 - 114s - loss: 0.0054 - accuracy: 0.6339 - val_loss: 1.8637 - val_accuracy: 0.6039 - 114s/epoch - 4s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff7e19b8950>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "callback = EarlyStopping(monitor='loss', patience=5)\n",
        "input1 = Input(shape=(trn_x_encoded2.shape[1],), name=\"input1\")\n",
        "input2 = Input(shape=(trn_x_sc2.shape[1],), name=\"input2\")\n",
        "input = concatenate([input1, input2], axis=1, name=\"concat_input\")\n",
        "x = Embedding(types_num+4, 256, mask_zero=True, input_length=max_len)(input)\n",
        "x = Bidirectional(LSTM(512, dropout=0.25), name=\"BiLSTM\")(x)\n",
        "x = Dense(128, activation = \"relu\", name=\"Dense_layer\")(x)\n",
        "x = Dropout(rate=0.25)(x)\n",
        "y = Dense(7, activation=\"sigmoid\", name=\"Output\")(x)\n",
        "model2 = Model(inputs= [input1, input2], outputs=y, name=\"Model2\")\n",
        "model2.summary()\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "model2.fit([trn_x_encoded2, trn_x_sc2], trn_y_encoded2, batch_size=32, epochs=100,\n",
        "           validation_split=0.2, callbacks=[callback], verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnu607e-NzLC"
      },
      "outputs": [],
      "source": [
        "pred2 = model2.predict([tst_x_encoded2, tst_x_sc2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJFw7XlgOGbS"
      },
      "outputs": [],
      "source": [
        "pred2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVNAmMjpdZVq"
      },
      "outputs": [],
      "source": [
        "pred2_binary = []\n",
        "for p in pred2:\n",
        "  p = [1 if i > 0.5 else 0 for i in p]\n",
        "  pred2_binary.append(p)\n",
        "pred2_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym0FbXm5OVqR"
      },
      "outputs": [],
      "source": [
        "f1_score2 = f1_score(tst_y_encoded2, pred2_binary, average=None)\n",
        "f1_score2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pcl_nn.ipynb",
      "provenance": [],
      "mount_file_id": "1x7YRANeUyimsukSgQXhSlbUKOkJOElcG",
      "authorship_tag": "ABX9TyN33d8ZMZq7yDEF77qgU5Es",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}